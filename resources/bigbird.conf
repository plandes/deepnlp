# transformer fixed (frozen embeddings)
[transformer_bigbird_resource]
class_name = zensols.deepnlp.transformer.TransformerResource
torch_config = instance: gpu_torch_config
model_id = google/bigbird-roberta-base
cased = True
# cache the model at the global level since there is only one huggingface
# transformer model we're using; also without it, subprocess CPU to CPU copy freezes:
# https://github.com/huggingface/transformers/issues/8649
cache = False
# from_pretrain extra arguments; speeds things up, but comment out for the
# first pretrained download
#args = dict: {'local_files_only': True}
# whether or not the embeddings are trainable (not frozen)
trainable = True

[transformer_bigbird_tokenizer]
class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
resource = instance: transformer_bigbird_resource
# the max number of word peice tokens; the word piece length is always the same
# or greater in count than linguistic tokens because the word piece algorithm
# tokenizes on characters; set to 0 set length to longest sentence per batch
word_piece_token_length = 0

[transformer_bigbird_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbedding
tokenizer = instance: transformer_bigbird_tokenizer
# uncomment to use the last layer (rather than the CLS token) as output
#output = last_hidden_state

[transformer_bigbird_feature_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingFeatureVectorizer
feature_id = transformer_bigbird
# encode at the (feature) document level
fold_method = concat_tokens
embed_model = instance: transformer_bigbird_embedding
# serialize (pickle) the decoded output to do the work up front
encode_transformed = ${deepnlp_default:transformer_encode_transformed}

[transformer_bigbird_embedding_layer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingLayer
embed_model = instance: transformer_bigbird_embedding
feature_vectorizer_manager = instance: language_vectorizer_manager
