## resources/download model
#
# 100 billion words X 300D
[word2vec_zip_resource]
class_name = zensols.install.Resource
# the following URL no longer works, this must be manually downloaded at:
# https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
url = https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
name = None
rename = False

[word2vec_installer]
class_name = zensols.install.Installer
downloader = object: zensols.install.Downloader
package_resource = zensols.deepnlp
resources = instance: list: word2vec_zip_resource

## embedding
#
[word2vec_300_embedding]
class_name = zensols.deepnlp.embed.Word2VecModel
installer = instance: word2vec_installer
resource = instance: word2vec_zip_resource
dimension = 300

[word2vec_300_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingFeatureVectorizer
feature_id = w2v300
# encode at the (feature) document level
fold_method = concat_tokens
# the word embedding model
embed_model = instance: word2vec_300_embedding
# do not serialize (pickle) the decoded output to do the work up front
encode_transformed = ${deepnlp_default:word2vec_encode_transformed}
# decode the embedding during the decode phase, which speeds things up since we
# GPU cache batches (model_settings:batch_iteration = gpu)
decode_embedding = True

[word2vec_300_embedding_layer]
class_name = zensols.deepnlp.layer.WordVectorEmbeddingLayer
embed_model = instance: word2vec_300_embedding
feature_vectorizer_manager = instance: language_vectorizer_manager
# freeze the embedding to train faster
trainable = ${deepnlp_default:word2vec_trainable}
