## Transformer expanders; this must be imported after `transformers.conf`

## Import mappings
[import]
sections = list: imp_trans_expander

[imp_trans_expander]
type = import
config_file = resource(zensols.deepnlp): resources/transformer-expander.yml


## Expanders
#
# expanders only works with embeddings utilizing all token level outputs
[transformer_fixed_embedding]
# uncomment to use the last layer (rather than the CLS token) as output
output = last_hidden_state

[transformer_trainable_embedding]
# uncomment to use the last layer (rather than the CLS token) as output
output = last_hidden_state

# dependency tree
[transformer_dep_expander_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerExpanderFeatureVectorizer
feature_id = tran_dep_expander
# encode at the (feature) document level
fold_method = concat_tokens
# the word embedding model
embed_model = instance: transformer_fixed_embedding
delegate_feature_ids = list: language_vectorizer_manager.dep
encode_transformed = False

# vectorized enumerated spaCy features
[transformer_enum_expander_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerExpanderFeatureVectorizer
feature_id = tran_enum_expander
# encode at the (feature) document level
fold_method = concat_tokens
# the word embedding model
embed_model = instance: transformer_fixed_embedding
delegate_feature_ids = list: language_vectorizer_manager.enum
# serialize (pickle) the decoded output to do the work up front
encode_transformed = False


## Vectorizer and managers
#
# a language specific vectorizer manager that uses the FeatureDocumentParser
# defined in `doc_parser` to create word embeddings using the vectorizer
# defined in `glove_50_feature_vectorizer` and natural language features
[transformer_expander_feature_manager]
class_name = zensols.deepnlp.vectorize.FeatureDocumentVectorizerManager
torch_config = instance: gpu_torch_config
doc_parser = instance: ${deepnlp_default:vectorizer_doc_parser}
# do not truncate tokens
token_length = -1
configured_vectorizers = list:
  transformer_dep_expander_vectorizer,
  transformer_enum_expander_vectorizer

# maintains a collection of all vectorizers for the framework
# don't clobber
# [vectorizer_manager_set]
# names = list: language_vectorizer_manager, transformer_expander_feature_manager


## Batch
#
[batch_dir_stash]
# feature grouping: when at least one in a group is needed, all of the features
# in that group are loaded
groups = eval: (
       # there will be N (batch_stash:batch_size) batch labels in one file in a
       # directory of just label files
       set('label'.split()),
       # because we might want to switch between embeddings, separate them
       set('glove_50_embedding'.split()),
       set('glove_300_embedding'.split()),
       set('word2vec_300_embedding'.split()),
       set('fasttext_news_300_embedding'.split()),
       set('fasttext_crawl_300_embedding'.split()),
       set('enums stats counts dependencies'.split()),
       set('transformer_trainable_embedding'.split()),
       set('transformer_fixed_embedding'.split()),
       set('transformer_sent_trainable_embedding'.split()),
       set('transformer_sent_fixed_embedding'.split()),
       set('transformer_enum_expander transformer_dep_expander'.split()))
