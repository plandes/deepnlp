## description: sentence BERT transformer

# model_id:
#
# The all-mpnet-base-v2 model provides the best quality, while all-MiniLM-L6-v2
# is 5 times faster and still offers good quality.
# https://www.sbert.net/docs/pretrained_models.html
#
# This resource library is provided as a convience in situtations where the
# ``transformer_fixed`` is also needed in the same project.  Otherise, the
# configurations are identical with the exception of the ``model_id`` property.

## Trainable (fine-tunable) transformer
#
# resource contains the transformer model details
[transformer_sent_trainable_resource]
class_name = zensols.deepnlp.transformer.TransformerResource
torch_config = instance: gpu_torch_config
model_id = sentence-transformers/all-mpnet-base-v2
#model_id = sentence-transformers/all-MiniLM-L6-v2
cased = False
# cache the model at the global level since there is only one huggingface
# transformer model we're using; also without it, subprocess CPU to CPU copy freezes:
# https://github.com/huggingface/transformers/issues/8649
cache = False
# whether or not the embeddings are trainable (not frozen)
trainable = True
# from_pretrain extra arguments; speeds things up
args = dict: {'local_files_only': ${deepnlp_default:transformer_local_files_only}}

[transformer_sent_trainable_tokenizer]
class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
resource = instance: transformer_sent_trainable_resource
# the max number of word peice tokens; the word piece length is always the same
# or greater in count than linguistic tokens because the word piece algorithm
# tokenizes on characters; set to 0 set length to longest sentence per batch
word_piece_token_length = ${deepnlp_default:word_piece_token_length}

[transformer_sent_trainable_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbedding
tokenizer = instance: transformer_sent_trainable_tokenizer

[transformer_sent_trainable_feature_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingFeatureVectorizer
feature_id = transformer_sent_trainable
# encode at the (feature) document level
fold_method = concat_tokens
embed_model = instance: transformer_sent_trainable_embedding
# trainable models must not encode/use frozen embeddings
encode_transformed = False
# whether to tokenize the document on encoding; set this to True only if the
# transformer_sent_trainable_resource:model_id will not change after vectorization
encode_tokenized = False

[transformer_sent_trainable_embedding_layer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingLayer
embed_model = instance: transformer_sent_trainable_embedding
feature_vectorizer_manager = instance: language_vectorizer_manager



## Transformer fixed (frozen embeddings)
#
# resource contains the transformer model details
[transformer_sent_fixed_resource]
class_name = zensols.deepnlp.transformer.TransformerResource
torch_config = instance: gpu_torch_config
model_id = sentence-transformers/all-mpnet-base-v2
#model_id = sentence-transformers/all-MiniLM-L6-v2
cased = False
# cache the model at the global level since there is only one huggingface
# transformer model we're using; also without it, subprocess CPU to CPU copy freezes:
# https://github.com/huggingface/transformers/issues/8649
cache = False
# from_pretrain extra arguments; speeds things up
args = dict: {'local_files_only': ${deepnlp_default:transformer_local_files_only}}
# whether or not the embeddings are trainable (not frozen)
trainable = False

[transformer_sent_fixed_tokenizer]
class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
resource = instance: transformer_sent_fixed_resource
# the max number of word peice tokens; the word piece length is always the same
# or greater in count than linguistic tokens because the word piece algorithm
# tokenizes on characters; set to 0 set length to longest sentence per batch
word_piece_token_length = ${deepnlp_default:word_piece_token_length}

[transformer_sent_fixed_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbedding
tokenizer = instance: transformer_sent_fixed_tokenizer

[transformer_sent_fixed_feature_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingFeatureVectorizer
feature_id = transformer_sent_fixed
# encode at the (feature) document level
fold_method = concat_tokens
embed_model = instance: transformer_sent_fixed_embedding
# serialize (pickle) the decoded output to do the work up front
encode_transformed = ${deepnlp_default:transformer_encode_transformed}
# whether to tokenize the document on encoding; set this to True only if the
# transformer_trainable_resource:model_id will not change after vectorization;
# this must be True when encode_transformed is True.
encode_tokenized = True

[transformer_sent_fixed_embedding_layer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingLayer
embed_model = instance: transformer_sent_fixed_embedding
feature_vectorizer_manager = instance: language_vectorizer_manager
