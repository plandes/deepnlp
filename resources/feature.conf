## Install the corpus
#
# declare resources to be downloaded
[feature_resource]
class_name = zensols.install.Resource
url = https://example.com

# the installer downloads and uncompresses the files
[feature_installer]
class_name = zensols.install.Installer
downloader = object: zensols.install.Downloader
base_directory = path: ${deepnlp_default:corpus_dir}
resources = instance: list: feature_resource


## Feature creation
#
# overrides AutoSplitDataframeStash to create the dataframe from parsing the
# sentences from the corups; the parent class the splits the dataframe in to
# datasets using a column that gives the dataset name
[dataframe_stash]
class_name = zensols.dataframe.ResourceFeatureDataframeStash
installer = instance: feature_installer
resource = instance: feature_resource
dataframe_path = path: ${default:data_dir}/feature/dataframe-stash.dat

# a stash that uses a directory of pickled files to store the parsed (POS tags,
# NER tagged entities, etc.), which are later used to create features
[feature_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:data_dir}/feature/parse

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing
[feature_factory_stash]
class_name = zensols.deepnlp.feature.DataframeDocumentFeatureStash
delegate = instance: feature_dir_stash
factory = instance: dataframe_stash
vec_manager = instance: language_vectorizer_manager
chunk_size = 0
workers = 0
#text_column = text

# create stratified splits across labels with the same distribution across each
# data set split
[feature_split_key_container]
class_name = zensols.dataset.StratifiedStashSplitKeyContainer
# the stash to stratify
stash = instance: feature_factory_stash
# distribution of each data set
distribution = dict: {'train': 0.8, 'test': 0.1, 'validation': 0.1}
# per dataset file name with newline separated keys
pattern = {name}.txt
key_path = path: ${deepnlp_default:corpus_dir}/dataset-row-ids
split_labels_path = path: ${default:data_dir}/feature/split-keys.dat
# the attribute on each data point (movie review) to create the stratas
partition_attr = label
# show per strata statistics (takes longer on `write`)
stratified_write = False

# a stash that splits along dataset type (i.e. train, validation, test)
[feature_stash]
class_name = zensols.dataset.DatasetSplitStash
delegate = instance: feature_factory_stash
split_container = instance: feature_split_key_container
