## Model configuration, settings and hyperparameters

# the executor uses PyTorch to train, test and validate the model itself; it
# also saves the results and model
[executor]
class_name = zensols.deeplearn.model.ModelExecutor
# human readable text
model_name = eval: 'NER: ${language_defaults:embedding}'.replace('_embedding', ' ')
# configures the model
model_settings = instance: model_settings
# configures the neural network
net_settings = instance: net_settings
# stash to get the batch data
dataset_stash = instance: sent_dataset_stash
# the datasets by name found in `dataset_stash`
# *Important:* order must be: training, validation/development, test
dataset_split_names = ${corpus_defaults:corpus_split_names}
# the pah the store the results after completing training or testing
result_path = path: ${default:results_dir}/model
# add results while training the model
intermediate_results_path = path: ${default:temporary_dir}/tmpres
# use regression error metrics to evaluate the results
#reduce_outcomes = none
# the path to watch for early stopping
update_path = path: ${default:temporary_dir}/update.json


## user tweeks

[linear_settings]
class_name = zensols.deeplearn.layer.DeepLinearNetworkSettings
# number deep linear layers configured after the LSTM
middle_features = eval: []
# number of output features
out_features = None
# the number of input features to the deep linear layer; set to null since
# calculated in the model
in_features = None
# whether to treat each middle layer as a scalar multiplier of the previous or
# to interpret them as a constant number of parameters
proportions = True
# number of times to repeat the middle layers
repeats = 1
# drop out used for the middle layers (set by root level settings)
dropout = None
# activation
activation = ${model_defaults:activation}
# 1d batch normalize
batch_norm_d = None
batch_norm_features = None
#batch_norm_features = ${language_defaults:token_length}

[recurrent_crf_settings]
class_name = zensols.deeplearn.layer.RecurrentCRFNetworkSettings
# the type of network (one of `rnn`, `lstm`, `gru`)
network_type = lstm
# the input size, but set to None since this is set from the embedding layer
# metadata
input_size = None
# hidden LSTM dimension
hidden_size = 24
# "stacked" LSTM
num_layers = 1
# number of output features
num_labels = eval: len((${category_settings:ent}))
# whether or the LSTM captures state in both directions
bidirectional = True
# decoder layer
decoder_settings = instance: linear_settings
# how the scores are returned
score_reduction = sum
# set by root level settings
dropout = ${model_defaults:dropout}
# no activation used in this set of layers
activation = ${model_defaults:activation}
# 1d batch normalize
batch_norm_d = None
batch_norm_features = None
#batch_norm_features = ${language_defaults:token_length}

# the network configuration, which contains constant information (as opposed to
# dynamic configuration such as held back `stash:decoded_attributes`)
[net_settings]
class_name = zensols.deepnlp.layer.EmbeddedRecurrentCRFSettings
# embedding layer used as the input layer
embedding_layer = instance: ${language_defaults:embedding}_layer
# metadata factory helps configure the network (see that configuration)
batch_metadata_factory = instance: sent_batch_metadata_factory
# the recurrent neural network after the embeddings
recurrent_crf_settings = instance: recurrent_crf_settings
# mask attribute
mask_attribute = mask

# model specific configuration, mutually exclusive from neural network details
[model_settings]
class_name = zensols.deeplearn.model.ModelSettings
# path where the model is saved on each validation decrease
path = path: ${default:temporary_dir}/model/${language_defaults:embedding}
# learning rate set on the optimizer
learning_rate = 0.001
# how the batches are buffered; one of `gpu`, which buffers all data in the
# GPU, `cpu`, which means keep all batches in CPU memory (the default), or
# `buffered` which means to buffer only one batch at a time (only for *very*
# large data) how to batch data: gpu to load all in to the GPU,
#
# its a tiny data set so it will literally all fit in GPU memory
batch_iteration = gpu
# number of epochs to train the model
epochs = 2
# the maximum number of times the validation loss can decrease per epoch before
# the executor "gives up" and ends training
#max_consecutive_increased_count = 5
# indicates the frequency by with the Python garbage collector should be
# invoked: 0: never 1: before and after training or testing 2: after each epoch
# 3: after each batch
#gc_level = 1
# optimizer
#optimizer_class_name = torch.optim.SGD
#criterion_class_name = torch.nn.MSELoss
#nominal_labels = False
# used a scored batch iterator to handle terminating CRF states
batch_iteration_class_name = zensols.deeplearn.model.ScoredBatchIterator
# leave CRF decoded output alone
reduce_outcomes = none
# learning rate scheduler
scheduler_class_name = torch.optim.lr_scheduler.ReduceLROnPlateau
# number of batches to limit for train, test and validation, which is used for
# debugging
#batch_limit = 1
