
## Batch and dataset stashes

[sent_batch_settings]
# the number of data instances per batch, and the first dimension of each
# tensor given to the model
batch_size = 32
# path to all batch keys/data
path = ${default:batch_data_dir}/b${batch_size}

# a stash that groups features across directories, with each directory
# containing batch files of the respective feature group
[sent_batch_dir_stash]
class_name = zensols.deeplearn.batch.BatchDirectoryCompositeStash
# top level directory to store each feature sub directory
path = path: ${sent_batch_settings:path}/data
groups = eval: (
	# there will be N (batch_stash:batch_size) batch labels in one file in a
	# directory of just label files
	set('ents mask'.split()),
	set('syns tags'.split()),
	# we might want to switch between embeddings, separate them
	set('glove_50_embedding'.split()),
	set('glove_300_embedding'.split()),
	set('word2vec_300_embedding'.split()),
	set('tags_expander syns_expander'.split()),
	set('ents_trans'.split()),
	set('transformer_trainable_embedding'.split()))

# a stash of Batch instances given to the model during training, validation and
# testing; this class spawns sub processes to concatenate arrays of features in
# to batches containing the tensors consumed by the model
[sent_batch_stash]
class_name = zensols.deeplearn.batch.BatchStash
delegate = instance: sent_batch_dir_stash
# this stash is used to generate instances of what will be used to create batches
split_stash_container = instance: sent_stash
# where to store the keys as mutually exclusive across dataset (train vs test etc)
data_point_id_sets_path = path: ${sent_batch_settings:path}/keys.dat
# indicate what will be used to vectorize in to tensors from features
vectorizer_manager_set = instance: sent_vectorizer_manager_set
# the class that contains the feature data, one for each data instance
data_point_type = eval({'import': ['ner']}): ner.NERDataPoint
# the class taht contains the batch data, which will have N instances of
# `data_point_type` where N is the `batch_size`
batch_type = eval({'import': ['ner']}): ner.NERBatch
# the attributes used only on loading; all features indicated in the vectorizer
# manager (i.e. `language_feature_manager`) are saved; this makes it fast to
# try different feature combinations without havign to re-vectorize the entire
# dataset; if this is set to `None`, use all attributes given
#
# train time tweakable
decoded_attributes = eval: set([
  # label (named entity)
  'ents',
  # mask on the named entity labels used by the LSTM+CRF
  'mask',
  # the fourth the named entity tag
  'tags',
  # a syntactic chunk tag
  'syns',
  # expanded tags for the transformer
  'tags_expander', 'syns_expander',
  # the embedding to use (one of many defined in `embeddings.conf`)
  '${language_defaults:embedding}'])

# the PyTorch configuration used to load batch data in to memory, in this case,
# the GPU if available
model_torch_config = instance: gpu_torch_config
# number of chunks of data given to each worker sub process; if 0 optimize for
# batch size and number of CPU cores
chunk_size = 0
# number sub processes; if 0, then the number of CPU cores; workers set to 3
# since google pretrained embeddings eat a lot of memory; and instance for each
# process
#
# on a machine with 64G main memory, word2vec batch creation maxes the memory
# at 3 subprocesses/workers
workers = 2
#workers = 1
# the number of data instances per batch, and the first dimension of each
# tensor given to the model
batch_size = ${sent_batch_settings:batch_size}
#batch_size = 4
# limit on the number of batches (when creating batchs) per data set; typically
# multiply this by 3 to get a total count
#batch_limit = 50

# stash to not only split data by dataset (i.e. train, test), but also sort the
# keys across all; which is important for reproducibility of results; this
# pulls from the `batch_stash`, so sorting is done only on the loaded data
[sent_dataset_stash]
class_name = zensols.dataset.SortedDatasetSplitStash
delegate = instance: sent_batch_stash
split_container = instance: sent_batch_stash
sort_function = eval: int

# produces metadata configured and discovered in the `batch_stash` to tell the
# model of the dimensionality of given when creating the network
[sent_batch_metadata_factory]
class_name = zensols.deeplearn.batch.BatchMetadataFactory
stash = instance: sent_batch_stash
