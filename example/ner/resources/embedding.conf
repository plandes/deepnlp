## Embedding

# glove embeddding model (not layer)
[glove_50_embedding]
class_name = zensols.deepnlp.embed.GloveWordEmbedModel
path = path: ${default:corpus_dir}/glove
desc = 6B
dimension = 50
lowercase = True

# a vectorizer that turns tokens (TokensContainer) in to indexes given to the
# embedding layer
[glove_50_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingFeatureVectorizer
# the feature id is used to connect instance data with the vectorizer used to
# generate the feature at run time
feature_id = wvglove50
embed_model = instance: glove_50_embedding
# do not serialize (pickle) the decoded output to do the work up front
encode_transformed = False

# a torch.nn.Module implementation that uses the an embedding model
[glove_50_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: glove_50_embedding
feature_vectorizer = instance: language_feature_manager

# glove 300 dim
[glove_300_embedding]
class_name = zensols.deepnlp.embed.GloveWordEmbedModel
path = path: ${default:glove_dir}
desc = 6B
dimension = 300
lowercase = True

[glove_300_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingFeatureVectorizer
feature_id = wvglove300
embed_model = instance: glove_300_embedding
# do not serialize (pickle) the decoded output to do the work up front
encode_transformed = False

[glove_300_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: glove_300_embedding
feature_vectorizer = instance: language_feature_manager

# word2vec
[word2vec_300_embedding]
class_name = zensols.deepnlp.embed.Word2VecModel
path = path: ${default:w2v_path}
dimension = 300

[word2vec_300_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingFeatureVectorizer
feature_id = w2v300
embed_model = instance: word2vec_300_embedding
# do not serialize (pickle) the decoded output to do the work up front
encode_transformed = False
# decode the embedding during the decode phase, which speeds things up since we
# GPU cache batches (model_settings:batch_iteration = gpu)
decode_embedding = False

[word2vec_300_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: word2vec_300_embedding
feature_vectorizer = instance: language_feature_manager


# transformer: fixed
[transformer_fixed_resource]
class_name = zensols.deepnlp.transformer.TransformerResource
torch_config = instance: gpu_torch_config
model_id = bert-base-cased
cased = True
# cache the model at the global level since there is only one huggingface
# transformer model we're using; also without it, subprocess CPU to CPU copy freezes:
# https://github.com/huggingface/transformers/issues/8649
cache = True
# whether or not the embeddings are trainable (not frozen)
trainable = False

[transformer_fixed_tokenizer]
class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
resource = instance: transformer_fixed_resource
# the max number of word peice tokens; the word piece length is always the same
# or greater in count than linguistic tokens because the word piece algorithm
# tokenizes on characters
word_piece_token_length = ${language_defaults:token_length}

[transformer_fixed_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbedding
tokenizer = instance: transformer_fixed_tokenizer
output = last_hidden_state

[transformer_fixed_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.TransformerEmbeddingFeatureVectorizer
feature_id = transformer_fixed
embed_model = instance: transformer_fixed_embedding
# serialize (pickle) the decoded output to do the work up front
encode_transformed = True

[transformer_fixed_embedding_layer]
class_name = zensols.deepnlp.vectorize.TransformerEmbeddingLayer
embed_model = instance: transformer_fixed_embedding
feature_vectorizer = instance: language_feature_manager


# transformer: trainable
[transformer_trainable_resource]
class_name = zensols.deepnlp.transformer.TransformerResource
torch_config = instance: gpu_torch_config
model_id = bert-base-cased
cased = True
# cache the model at the global level since there is only one huggingface
# transformer model we're using; also without it, subprocess CPU to CPU copy freezes:
# https://github.com/huggingface/transformers/issues/8649
cache = True
# whether or not the embeddings are trainable (not frozen)
trainable = True

[transformer_trainable_tokenizer]
class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
resource = instance: transformer_trainable_resource
# the max number of word peice tokens; the word piece length is always the same
# or greater in count than linguistic tokens because the word piece algorithm
# tokenizes on characters
word_piece_token_length = ${language_defaults:token_length}

[transformer_trainable_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbedding
tokenizer = instance: transformer_trainable_tokenizer
output = last_hidden_state

[transformer_trainable_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.TransformerEmbeddingFeatureVectorizer
feature_id = transformer_trainable
embed_model = instance: transformer_trainable_embedding
# serialize (pickle) the decoded output to do the work up front
encode_transformed = False

[transformer_trainable_embedding_layer]
class_name = zensols.deepnlp.vectorize.TransformerEmbeddingLayer
embed_model = instance: transformer_trainable_embedding
feature_vectorizer = instance: language_feature_manager
