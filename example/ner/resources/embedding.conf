## transformer
#
[transformer_defaults]
# from_pretrain extra arguments; speeds things up, but comment out for the
# first pretrained download
#args = dict: {'local_files_only': True}
args = dict: {}
# cache the model at the global level
cache = False

# transformer
[transformer_trainable_resource]
class_name = zensols.deepnlp.transformer.TransformerResource
torch_config = instance: gpu_torch_config
# bert won't fit in memory with this model (transformer + LSTM + decoder + CRF)
#model_id = distilbert-base-cased
model_id = bert-base-cased
#model_id = roberta-base
cased = True
# whether or not the embeddings are trainable (not frozen)
trainable = True
args = ${transformer_defaults:args}
cache = ${transformer_defaults:cache}}

[transformer_trainable_tokenizer]
class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
resource = instance: transformer_trainable_resource
# the max number of word peice tokens; the word piece length is always the same
# or greater in count than linguistic tokens because the word piece algorithm
# tokenizes on characters
word_piece_token_length = -1

[transformer_trainable_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbedding
tokenizer = instance: transformer_trainable_tokenizer
output = last_hidden_state

[transformer_trainable_feature_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingFeatureVectorizer
feature_id = transformer_trainable
# encode at the (feature) document level
encode_level = concat_tokens
# the word embedding model
embed_model = instance: transformer_trainable_embedding
# serialize (pickle) the decoded output to do the work up front
encode_transformed = False

[transformer_trainable_embedding_layer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingLayer
embed_model = instance: transformer_trainable_embedding
feature_vectorizer = instance: language_feature_manager
