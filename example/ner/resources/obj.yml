## Install the CoNLL-2003 corpus
#
# declare resources to be downloaded
feature_resource_dev:
  class_name: zensols.install.Resource
  url: 'https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/dev.txt'
  name: None
feature_resource_test:
  class_name: zensols.install.Resource
  url: 'https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/test.txt'
  name: None
feature_resource_train:
  class_name: zensols.install.Resource
  url: 'https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/train.txt'
  name: None

# the installer downloads and uncompresses the files
feature_installer:
  class_name: zensols.install.Installer
  downloader: 'object: zensols.install.Downloader'
  base_directory: 'path: ${deepnlp_default:corpus_dir}'
  resources: 'instance: list: feature_resource_dev, feature_resource_test, feature_resource_train'


## Corpus/feature creation
#
feature_dir_stash:
  class_name: zensols.persist.DirectoryStash
  path: 'path: ${default:data_dir}/feature/parsed'

# creates files of key IDs for each split
feature_factory_stash:
  class_name: ner.SentenceFactoryStash
  delegate: 'instance: feature_dir_stash'
  key_path: 'path: ${default:data_dir}/feature/split-keys'
  pattern: '{name}.dat'
  installer: 'instance: feature_installer'
#  dataset_limit: 10

# a stash that splits along dataset type (i.e. train, validation, test)
feature_stash:
  class_name: zensols.dataset.SortedDatasetSplitStash
  delegate: 'instance: feature_factory_stash'
  split_container: 'instance: feature_factory_stash'
  sort_function: 'eval: int'

# provides corpus statistics
feature_stats:
  class_name: ner.SentenceStatsCalculator
  stash: 'instance: feature_stash'
  path: 'path: ${default:data_dir}/feature/stats.dat'


## Language parsing
#
# language resource, which loads the SpacCy model and token normalizers
filter_token_mapper:
  remove_space: True

map_filter_token_normalizer:
  embed_entities: False

# creates features from documents by invoking by using SpaCy to parse the text
doc_parser:
  # remove empty sentences or sentences with only whitespace
  remove_empty_sentences: True
  # indicate which features to keep after the parsing
  token_feature_ids: 'set: sent_i, tag, tag_, idx'


# Vectorization
#
tok_label_1_vectorizer:
  categories: 'eval: (${category_settings:ent})'

tag_replace_vectorizer:
  class_name: 'zensols.deepnlp.vectorize.OneHotEncodedFeatureDocumentVectorizer'
  feature_id: 'tag'
  encode_transformed: 'False'
  optimize_bools: 'True'
  categories: 'eval: (${category_settings:tag})'
  feature_attribute: 'tag_'

syn_replace_vectorizer:
  class_name: 'zensols.deepnlp.vectorize.OneHotEncodedFeatureDocumentVectorizer'
  feature_id: 'syn'
  encode_transformed: 'False'
  optimize_bools: 'True'
  categories: 'eval: (${category_settings:syn})'
  feature_attribute: 'syn_'

language_feature_manager:
  configured_vectorizers:
    - glove_50_feature_vectorizer
    - glove_300_feature_vectorizer
    - word2vec_300_feature_vectorizer
    - transformer_trainable_feature_vectorizer
    - tag_replace_vectorizer
    - syn_replace_vectorizer


## Batch
#
conll_lang_batch_mappings:
  manager_mappings:
    - vectorizer_manager_name: language_feature_manager
      fields:
        - attr: tags
          feature_id: tag
          is_agg: true
          attr_access: doc
        - attr: syns
          feature_id: syn
          is_agg: true
          attr_access: doc

# batch mappings from attribute to feature IDs and which to use from resource
# libs
ner_batch_mappings:
  label_attribute_name: tok_labels
  batch_feature_mapping_adds:
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): tok_label_batch_mappings'
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): conll_lang_batch_mappings'
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): lang_batch_mappings'
  field_keep:
    - tok_labels
    - tok_mask
    - tags
    - syns
    - glove_50_embedding
#    - glove_300_embedding
#    - word2vec_300_embedding
#    - transformer_trainable_embedding

# a stash that groups features across directories, with each directory
# containing batch files of the respective feature group
batch_dir_stash:
  groups:
    # there will be N (batch_stash:batch_size) batch labels in one file in a
    # directory of just label files
    - [tok_labels, tok_mask]
    - [syns, tags]
    # we might want to switch between embeddings, separate them
    - [glove_50_embedding]
    - [glove_300_embedding]
    - [word2vec_300_embedding]
    - [tags_expander, syns_expander]
    - [ents_trans, transformer_trainable_embedding]

# map feature attributes (sections) to feature IDs to connect features to
# vectorizers
batch_stash:
  batch_feature_mappings: 'dataclass(zensols.deeplearn.batch.ConfigBatchFeatureMapping): ner_batch_mappings'
  # the class that contains the feature data, one for each data instance
  data_point_type: "eval({'import': ['ner']}): ner.NERDataPoint"
  decoded_attributes: 'set: tok_labels, tok_mask, syns, ${ner_default:embedding}'
  # use only two cores
  workers: -2
  batch_size: 32


## Model
#
executor:
  # the datasets by name found in `dataset_stash`
  # *Important:* order must be: training, validation/development, test
  dataset_split_names: [train, dev, test]
  # configures the neural network
  net_settings: 'instance: recurrent_crf_net_settings'

# declare the ModelFacade to use for the application
facade:
  class_name: zensols.deepnlp.classify.ClassifyModelFacade

model_settings:
  learning_rate: 0.001
  epochs: 35
