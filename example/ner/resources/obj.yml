## Install the CoNLL-2003 corpus
#
# declare resources to be downloaded
feature_resource_dev:
  class_name: zensols.install.Resource
  url: 'https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/dev.txt'
  name: None
feature_resource_test:
  class_name: zensols.install.Resource
  url: 'https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/test.txt'
  name: None
feature_resource_train:
  class_name: zensols.install.Resource
  url: 'https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/train.txt'
  name: None

# the installer downloads and uncompresses the files
feature_installer:
  class_name: zensols.install.Installer
  downloader: 'object: zensols.install.Downloader'
  base_directory: 'path: ${deepnlp_default:corpus_dir}'
  resources: 'instance: list: feature_resource_dev, feature_resource_test, feature_resource_train'


## Corpus/feature creation
#
feature_dir_stash:
  class_name: zensols.persist.DirectoryStash
  path: 'path: ${default:data_dir}/feature/parsed'

# creates files of key IDs for each split
feature_factory_stash:
  class_name: ner.SentenceFactoryStash
  delegate: 'instance: feature_dir_stash'
  key_path: 'path: ${default:data_dir}/feature/split-keys'
  pattern: '{name}.dat'
  installer: 'instance: feature_installer'
#  dataset_limit: 10

# a stash that splits along dataset type (i.e. train, validation, test)
feature_stash:
  class_name: zensols.dataset.SortedDatasetSplitStash
  delegate: 'instance: feature_factory_stash'
  split_container: 'instance: feature_factory_stash'
  sort_function: 'eval: int'

# provides corpus statistics
feature_stats:
  class_name: ner.SentenceStatsCalculator
  stash: 'instance: feature_stash'
  path: 'path: ${default:data_dir}/feature/stats.dat'


## Language parsing
#
# language resource, which loads the SpacCy model and token normalizers
filter_token_mapper:
  remove_space: True

map_filter_token_normalizer:
  embed_entities: False

# creates features from documents by invoking by using SpaCy to parse the text
doc_parser:
  # indicate which features to keep after the parsing
  token_feature_ids: 'set: sent_i, tag, tag_, idx'


## Vectorization
#
# provide the nominals for the token label vectorizer
tok_label_1_vectorizer:
  categories: 'eval: (${category_settings:ent})'

tag_replace_vectorizer:
  class_name: 'zensols.deepnlp.vectorize.OneHotEncodedFeatureDocumentVectorizer'
  feature_id: tag
  encode_transformed: false
  optimize_bools: true
  categories: 'eval: (${category_settings:tag})'
  feature_attribute: tag_

syn_replace_vectorizer:
  class_name: 'zensols.deepnlp.vectorize.OneHotEncodedFeatureDocumentVectorizer'
  feature_id: 'syn'
  encode_transformed: false
  optimize_bools: true
  categories: 'eval: (${category_settings:syn})'
  feature_attribute: 'syn_'

language_vectorizer_manager:
  configured_vectorizers:
    - tag_replace_vectorizer
    - syn_replace_vectorizer
    - glove_50_feature_vectorizer
    - glove_300_feature_vectorizer
    - word2vec_300_feature_vectorizer
    - transformer_trainable_feature_vectorizer
    - tok_label_transformer_vectorizer


## Batch
#
conll_lang_batch_mappings:
  manager_mappings:
    - vectorizer_manager_name: language_vectorizer_manager
      fields:
        - attr: tags
          feature_id: tag
          is_agg: true
          attr_access: doc
        - attr: syns
          feature_id: syn
          is_agg: true
          attr_access: doc
        - attr: tok_label_transformer
          feature_id: tclabel_trans
          is_agg: true
          attr_access: trans_doc
          is_label: true

# batch mappings from attribute to feature IDs and which to use from resource
# libs
ner_batch_mappings:
  condition:
    if: "eval: '${ner_default:name}'.startswith('transformer')"
    then:
      label_attribute_name: tok_label_transformer
    else:
      label_attribute_name: tok_labels
  batch_feature_mapping_adds:
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): tok_label_batch_mappings'
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): conll_lang_batch_mappings'
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): lang_batch_mappings'
  field_keep:
    - tok_labels
    - tok_mask
    - tags
    - syns
    - glove_50_embedding
    - glove_300_embedding
    - word2vec_300_embedding
    - transformer_trainable_embedding
    - tok_label_transformer

# a stash that groups features across directories, with each directory
# containing batch files of the respective feature group
batch_dir_stash:
  groups:
    # there will be N (batch_stash:batch_size) batch labels in one file in a
    # directory of just label files
    - [tok_labels, tok_mask]
    - [syns, tags]
    # we might want to switch between embeddings, separate them
    - [glove_50_embedding]
    - [glove_300_embedding]
    - [word2vec_300_embedding]
    - [tok_label_transformer, transformer_trainable_embedding]

# map feature attributes (sections) to feature IDs to connect features to
# vectorizers
batch_stash:
  # mappings from readable attribute names to vectorizer feature IDs
  batch_feature_mappings: 'dataclass(zensols.deeplearn.batch.ConfigBatchFeatureMapping): ner_batch_mappings'
  # the class that contains the feature data, one for each data instance
  data_point_type: "eval({'import': ['ner']}): ner.NERDataPoint"
  decoded_attributes: 'set: tok_labels, tok_mask, syns, ${ner_default:embedding}'
  # use only two cores
  workers: 2
  batch_size: 32


## Model
#
executor:
  # the datasets by name found in `dataset_stash`
  # *Important:* order must be: training, validation/development, test
  dataset_split_names: [train, dev, test]
  # configures the neural network
  net_settings: 'instance: recurrent_crf_net_settings'

## Prediction
#
# create data points from the client
feature_prediction_mapper:
  class_name: ner.NERPredictionMapper

# learning rate and num epochs
model_settings:
  learning_rate: 0.001
  epochs: 40


## Transformer
#
# embedding
transformer_trainable_embedding:
  output: last_hidden_state

# vectorizer
tok_label_transformer_vectorizer:
  class_name: zensols.deepnlp.transformer.TransformerNominalFeatureVectorizer
  feature_id: tclabel_trans
  delegate_feature_id: tok_label_vectorizer_manager.tclabel1
  fold_method: concat_tokens
  embed_model: 'instance: transformer_trainable_embedding'
  encode_transformed: false
  label_all_tokens: true
