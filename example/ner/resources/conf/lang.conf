## language and resources configuration

[language_defaults]
token_length = 70
embedding = glove_50_embedding


# language resource, which loads the SpacCy model and token normalizers
[langres]
class_name = zensols.nlp.LanguageResource
#token_normalizer = instance: token_normalizer

# creates features from documents by invoking by using SpaCy to parse the text
[doc_parser]
class_name = zensols.deepnlp.FeatureDocumentParser
langres = instance: langres
# indicate which features to keep after the parsing; if this is not given, all
# features are kept and persisted
#
# 'norm' is good for debuging, 'dep', 'children' and the rest are needed for
# dep head tree features
#token_feature_ids = eval: set('norm ent dep tag children i dep_ is_punctuation'.split())
token_feature_ids = eval: set('norm tag'.split())

# a language specific vectorizer manager that uses the FeatureDocumentParser
# defined in `doc_parser` to create word embeddings using the vectorizer
# defined in `glove_50_feature_vectorizer` and natural language features
[language_feature_manager]
class_name = zensols.deepnlp.vectorize.TokenContainerFeatureVectorizerManager
torch_config = instance: torch_config
# word embedding vectorizers can not be class level since each instance is
# configured
configured_vectorizers = eval: [
    'glove_50_feature_vectorizer',
    'glove_300_feature_vectorizer',
    'word2vec_300_feature_vectorizer',
    'bert_feature_vectorizer']
# used for parsing `FeatureDocument` instances
doc_parser = instance: doc_parser
# the number of tokens in the document to use
# token length is not one to one with parsed tokens when using BERT, 70 works well
token_length = ${language_defaults:token_length}
# tokens to use
token_feature_ids = ${doc_parser:token_feature_ids}



## Embedding

# glove embeddding model (not layer)
[glove_50_embedding]
class_name = zensols.deepnlp.embed.GloveWordEmbedModel
path = path: ${default:corpus_dir}/glove
desc = 6B
dimension = 50
lowercase = True

# a vectorizer that turns tokens (TokensContainer) in to indexes given to the
# embedding layer
[glove_50_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorSentenceFeatureVectorizer
# the feature id is used to connect instance data with the vectorizer used to
# generate the feature at run time
feature_id = wvglove50
embed_model = instance: glove_50_embedding
# treat the document as a stream of tokens generating a flat set of indexes
as_document = True
# decode in to the embedding matrix
#decode_embedding = True

# a torch.nn.Module implementation that uses the an embedding model
[glove_50_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: glove_50_embedding
feature_vectorizer = instance: language_feature_manager

# glove 300 dim
[glove_300_embedding]
class_name = zensols.deepnlp.embed.GloveWordEmbedModel
path = path: ${default:corpus_dir}/glove
desc = 6B
dimension = 300
lowercase = True

[glove_300_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorSentenceFeatureVectorizer
feature_id = wvglove300
embed_model = instance: glove_300_embedding
as_document = True

[glove_300_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: glove_300_embedding
feature_vectorizer = instance: language_feature_manager

# word2vec
[word2vec_300_embedding]
class_name = zensols.deepnlp.embed.Word2VecModel
path = path: ${default:corpus_dir}/word2vec/GoogleNews-vectors-negative300.bin
dimension = 300

[word2vec_300_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorSentenceFeatureVectorizer
feature_id = w2v300
embed_model = instance: word2vec_300_embedding
as_document = True
decode_embedding = True

[word2vec_300_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: word2vec_300_embedding
feature_vectorizer = instance: language_feature_manager

# bert
[bert_embedding]
class_name = zensols.deepnlp.embed.BertEmbeddingModel
torch_config = instance: gpu_torch_config
#model_name = distilbert
model_name = roberta
cache_dir = path: ${default:corpus_dir}/bert

[bert_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.BertSentenceFeatureVectorizer
feature_id = bert
embed_model = instance: bert_embedding
as_document = True

[bert_embedding_layer]
class_name = zensols.deepnlp.vectorize.BertEmbeddingLayer
embed_model = instance: bert_embedding
feature_vectorizer = instance: language_feature_manager
