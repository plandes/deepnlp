## Natural language and resources configuration

# defaults used in other configuration
[language_defaults]
# sentence token length; corpus is chunked as sentences to begin with
token_length = -1
# configure this in overrides given in ../models
embedding = ${language_defaults:embedding}

# language resource, which loads the SpacCy model and token normalizers
[filter_token_mapper]
class_name = zensols.nlp.FilterTokenMapper
remove_space = True

[token_normalizer]
class_name = zensols.nlp.MapTokenNormalizer
mapper_class_list = list: filter_token_mapper
embed_entities = False

# creates features from documents by invoking by using SpaCy to parse the text
[doc_parser]
class_name = zensols.nlp.SpacyFeatureDocumentParser
token_normalizer = instance: token_normalizer
# remove empty sentences or sentences with only whitespace, which happens with
# two space separated sentences starting with spaCey 3
remove_empty_sentences = True
# indicate which features to keep after the parsing; if this is not given, all
# features are kept and persisted
#
# 'norm' is good for debuging, 'dep', 'children' and the rest are needed for
# dep head tree features
token_feature_ids = eval: set('norm sent_i tag tag_ dep_ idx'.split())


# lang features
[tag_replace_vectorizer]
class_name = zensols.deepnlp.vectorize.OneHotEncodedFeatureDocumentVectorizer
feature_id = tag
encode_transformed = False
optimize_bools = True
categories = eval: (${category_settings:tag})
feature_attribute = tag_

[syn_replace_vectorizer]
class_name = zensols.deepnlp.vectorize.OneHotEncodedFeatureDocumentVectorizer
feature_id = syn
encode_transformed = False
optimize_bools = True
categories = eval: (${category_settings:syn})
feature_attribute = syn_


# expanders
[transformer_tag_expander_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerExpanderFeatureVectorizer
feature_id = transformer_tags_expander
encode_level = doc
embed_model = instance: transformer_trainable_embedding
delegate_feature_ids = list: tag
encode_transformed = False

[transformer_syn_expander_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerExpanderFeatureVectorizer
feature_id = transformer_syns_expander
encode_level = doc
embed_model = instance: transformer_trainable_embedding
delegate_feature_ids = list: syn
encode_transformed = False

[ent_label_trans_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerNominalFeatureVectorizer
feature_id = entlabel_trans
size = -1
delegate_feature_id = label_vectorizer_manager.entlabel1
encode_level = doc
embed_model = instance: transformer_trainable_embedding
encode_transformed = False
label_all_tokens = True


# a language specific vectorizer manager that uses the FeatureDocumentParser
# defined in `doc_parser` to create word embeddings using the vectorizer
# defined in `glove_50_feature_vectorizer` and natural language features
[language_feature_manager]
class_name = zensols.deepnlp.vectorize.FeatureDocumentVectorizerManager
torch_config = instance: gpu_torch_config
# word embedding vectorizers can not be class level since each instance is
# configured
configured_vectorizers = list:
    glove_50_feature_vectorizer,
    glove_300_feature_vectorizer,
    word2vec_300_feature_vectorizer,
    tag_replace_vectorizer,
    syn_replace_vectorizer,
    transformer_trainable_feature_vectorizer,
    transformer_tag_expander_vectorizer,
    transformer_syn_expander_vectorizer,
    ent_label_trans_vectorizer

# used for parsing `FeatureDocument` instances
doc_parser = instance: doc_parser
# the number of tokens in the document to use
# token length is not one to one with parsed tokens when using BERT, 70 works well
token_length = ${language_defaults:token_length}
# tokens to use
token_feature_ids = ${doc_parser:token_feature_ids}
