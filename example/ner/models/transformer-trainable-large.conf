[embedding_constants]
name = transformer_trainable_embedding

[language_defaults]
embedding = ${embedding_constants:name}

# overridding the batch size has to either be copied to all configurations like
# language_defaults:embedding or overridden in all places mentioned in the
# configuration
[sent_batch_settings]
batch_size = 16
path = ${default:batch_data_dir}/b${batch_size}

[sent_batch_dir_stash]
path = path: ${sent_batch_settings:path}/data

[sent_batch_stash]
decoded_attributes = eval: set([
  # label (named entity)
  'ents_trans',
  # the embedding to use (one of many defined in `embeddings.conf`)
  '${language_defaults:embedding}'])

[trans_decoder_settings]
class_name = zensols.deeplearn.layer.DeepLinearNetworkSettings
# number deep linear layers configured after the LSTM
middle_features = eval: []
# number of output features
out_features = ${label_constants:n_labels}
# the number of input features to the deep linear layer; set to null since
# calculated in the model
in_features = None
# whether to treat each middle layer as a scalar multiplier of the previous or
# to interpret them as a constant number of parameters
proportions = True
# number of times to repeat the middle layers
repeats = 1
# drop out used for the middle layers (set by root level settings)
dropout = 0.1
# activation
activation = None
# 1d batch normalize
batch_norm_d = None
batch_norm_features = None

[trans_net_settings]
class_name = zensols.deepnlp.transformer.TransformerSequenceNetworkSettings
# embedding layer used as the input layer
embedding_layer = instance: ${embedding_constants:name}_layer
# dropout used on the output from the embedding layer
dropout = 0.1
# single fully connected linear decoder layer
decoder_settings = instance: trans_decoder_settings
# metadata factory helps configure the network (see that configuration)
batch_metadata_factory = instance: sent_batch_metadata_factory

[transformer_trainable_resource]
model_id = bert-large-cased

[executor]
net_settings = instance: trans_net_settings
# human readable text
model_name = transformer_trainable_large

[model_settings]
batch_iteration = cpu
epochs = 5
optimizer_class_name = zensols.deepnlp.transformer.TransformerAdamFactory
# huggingface example
learning_rate = eval: 5e-5
optimizer_params = dict: {'eps': 1e-8}
