[embedding_constants]
name = transformer_trainable_embedding

[language_defaults]
embedding = ${embedding_constants:name}

[sent_batch_stash]
decoded_attributes = eval: set([
  # label (named entity)
  'ents_trans',
  # expanded tags for the transformer
  '${language_defaults:embedding}'])

[trans_decoder_settings]
class_name = zensols.deeplearn.layer.DeepLinearNetworkSettings
# number deep linear layers configured after the LSTM
middle_features = eval: []
# number of output features
out_features = ${label_constants:n_labels}
# the number of input features to the deep linear layer; set to null since
# calculated in the model
in_features = None
# whether to treat each middle layer as a scalar multiplier of the previous or
# to interpret them as a constant number of parameters
proportions = True
# number of times to repeat the middle layers
repeats = 1
# drop out used for the middle layers (set by root level settings)
dropout = 0.1
# activation
activation = None
# 1d batch normalize
batch_norm_d = None
batch_norm_features = None

[trans_net_settings]
class_name = zensols.deepnlp.transformer.TransformerSequenceNetworkSettings
# embedding layer used as the input layer
embedding_layer = instance: ${embedding_constants:name}_layer
# dropout used on the output from the embedding layer
dropout = 0.1
# single fully connected linear decoder layer
decoder_settings = instance: trans_decoder_settings
# metadata factory helps configure the network (see that configuration)
batch_metadata_factory = instance: sent_batch_metadata_factory

[executor]
net_settings = instance: trans_net_settings

[model_settings]
epochs = 4
learning_rate = eval: 5e-5
optimizer_class_name = zensols.deepnlp.transformer.TransformerAdamFactory
optimizer_params = dict: {'eps': 1e-8}
#scheduler_class_name = zensols.deepnlp.transformer.TransformerSchedulerFactory
#scheduler_params = dict: {'name': 'linear', 'num_warmup_steps': 500}
