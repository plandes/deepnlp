[language_defaults]
embedding = transformer_trainable_embedding

[sent_batch_stash]
# train time tweakable
decoded_attributes = eval: set([
  # label (named entity)
  'ents',
  # mask on the named entity labels used by the LSTM+CRF
  'mask',
  # expanded tags for the transformer
#  'tags_expander', 'syns_expander',
  # the embedding to use (one of many defined in `embeddings.conf`)
  '${language_defaults:embedding}'])

[model_settings]
# number of epochs to train the model
epochs = 60
# how the batches are buffered; one of `gpu`, which buffers all data in the
# GPU, `cpu`, which means keep all batches in CPU memory (the default), or
# `buffered` which means to buffer only one batch at a time (only for *very*
# large data) how to batch data: gpu to load all in to the GPU,
#batch_iteration = cpu
