{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning framework example: Named Entity Recognition\n",
    "\n",
    "This notebook demonstrates how to use the deeplearning API to train and test the model on the [CoNNL 2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/).  The task is to identify named entities (i.e. people, organizations etc).  See the [saved version](https://htmlpreview.github.io/?https://github.com/plandes/deepnlp/blob/master/example/ner/notebook/ner.html) for output.\n",
    "\n",
    "**Important**: Please see the [Movie Review notebook example](https://github.com/plandes/deepnlp/blob/master/example/movie/notebook/movie.ipynb) for a more simple example of how to utilize the framework to tune pamateres in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environemnt configuration and set up: add this (deepnlp) library to the Python path and framework entry point\n",
    "from mngfac import JupyterManagerFactory\n",
    "fac = JupyterManagerFactory()\n",
    "mng = fac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print information about \n",
    "\n",
    "Use the factory to create the model executor.  The `write` method gives statistics on the data set that is configured on the executor.  Note that the first time this runs, the framework automatically downloads the corpus, vectorizers and creates batches for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "facade = mng.create_facade()\n",
    "mng.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters\n",
    "\n",
    "Set model parameters to get a feel for where they need to be before changing features.  Start with Glove 50 dimensional word embeddings with a learning rate of 0.01 and 8 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.learning_rate = 0.01\n",
    "facade.epochs = 8\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove 300 embeddings\n",
    "\n",
    "Next we use the same learning rate, but switch to the 300 dimension version of the embeddings.  The number of epochs is reduced because I have run the test before I know at what epoch the validation loss converges.  Since the model is saved only when the validation loss decreases, we early stop at 6 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.epochs = 6\n",
    "facade.embedding = 'glove_300_embedding'\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec Embeddings\n",
    "\n",
    "Now we switch to the Google 300D word2vec pretrained vectors using 12 epochs, even though it has converged at 9 epochs previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.embedding = 'word2vec_300_embedding'\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Embeddings\n",
    "\n",
    "Now we test with Bert context aware frozen (not trainable) embeddings using 10 epochs.  We must empty the `net_settings` attributes, which are the lingustic features, since Bert tokenizes using the word piece algorithm and the tensor shapes will not align.  We'll address this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer')\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer', model_id='roberta-base')\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer', model_id='bert-large-cased')\n",
    "facade.epochs = 8\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('zensols.deepnlp.transformer.optimizer').setLevel(logging.INFO)\n",
    "mng.config('model_settings',\n",
    "           scheduler_class_name='zensols.deepnlp.transformer.TransformerSchedulerFactory',\n",
    "           scheduler_params=\"dict: {'name': 'linear', 'num_warmup_steps': 0.01}\")\n",
    "facade = mng.create_facade('transformer', model_id='bert-large-cased')\n",
    "facade.epochs = 22\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results\n",
    "\n",
    "Generate a dataframe with the performance metrics of the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zensols.deeplearn.result import ModelResultManager, ModelResultReporter\n",
    "rm: ModelResultManager = facade.result_manager\n",
    "reporter = ModelResultReporter(rm, include_validation=False)\n",
    "reporter.dataframe.drop(columns=['name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
