{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning framework example: Named Entity Recognition\n",
    "\n",
    "This notebook demonstrates how to use the deeplearning API to train and test the model on the [CoNNL 2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/).  The task is to identify named entities (i.e. people, organizations etc).  See the [saved version](https://htmlpreview.github.io/?https://github.com/plandes/deepnlp/blob/master/example/ner/notebook/ner.ipynb) for output.\n",
    "\n",
    "**Important**: Please see the Movie Review notebook example in the `zensols.movie` API first, as it contains more explaination of how the framework is used.  The purpose of this notebook is to run the MNIST dataset and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harness import NotebookHarness\n",
    "harness = NotebookHarness()\n",
    "mng = harness()\n",
    "#mng = harness(1, 'target2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print information about \n",
    "\n",
    "Use the factory to create the model executor.  The `write` method gives statistics on the data set that is configured on the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zensols.config import Writable\n",
    "# set indention level for human readable (pretty print) output\n",
    "Writable.WRITABLE_INDENT_SPACE = 2\n",
    "facade = mng.create_facade('glove50')\n",
    "facade.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the model\n",
    "\n",
    "Train and test the model with the default (low) number of epochs to make sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.epochs = 2\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters\n",
    "\n",
    "Set model parameters to get a feel for where they need to be before changing features.  Start with Glove 50 dimensional word embeddings with a learning rate of 0.01 and 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.learning_rate = 0.01\n",
    "facade.epochs = 10\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove 300 embeddings\n",
    "\n",
    "Next we use the same learning rate, but switch to the 300 dimension version of the embeddings.  The number of epochs is reduced because I have run the test before I know at what epoch the validation loss converges.  Since the model is saved only when the validation loss decreases, we early stop at 8 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.epochs = 8\n",
    "facade.embedding = 'glove_300_embedding'\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Embeddings\n",
    "\n",
    "Now we switch to the Google 300D word2vec pretrained vectors using 12 epochs, even though it has converged at 9 epochs previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.epochs = 12\n",
    "facade.embedding = 'word2vec_300_embedding'\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Embeddings\n",
    "\n",
    "Now we test with Bert context aware frozen (not trainable) embeddings using 10 epochs.  We must empty the `net_settings` attributes, which are the lingustic features, since Bert tokenizes using the word piece algorithm and the tensor shapes will not align.  We'll address this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer-trainable')\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mng.clear()\n",
    "facade = mng.create_facade('transformer-trainable')\n",
    "facade.net_settings.add_attributes = ('syns_expander', 'tags_expander')\n",
    "facade.epochs = 5\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mng.clear()\n",
    "mng.config('transformer_trainable_resource', model_id='roberta-base')\n",
    "facade = mng.create_facade('transformer-trainable')\n",
    "facade.epochs = 8\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('zensols.deepnlp.transformer.optimizer').setLevel(logging.INFO)\n",
    "mng.clear()\n",
    "mng.config('model_settings',\n",
    "           scheduler_class_name='zensols.deepnlp.transformer.TransformerSchedulerFactory',\n",
    "           scheduler_params=\"dict: {'name': 'linear', 'num_warmup_steps': 0.01}\")\n",
    "facade = mng.create_facade('transformer-trainable')\n",
    "facade.epochs = 5\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng.clear()\n",
    "facade = mng.create_facade('transformer-trainable-large')\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('zensols.deepnlp.transformer.optimizer').setLevel(logging.INFO)\n",
    "mng.clear()\n",
    "mng.config('model_settings',\n",
    "           scheduler_class_name='zensols.deepnlp.transformer.TransformerSchedulerFactory',\n",
    "           scheduler_params=\"dict: {'name': 'linear', 'num_warmup_steps': 0.01}\")\n",
    "facade = mng.create_facade('transformer-trainable-large')\n",
    "facade.epochs = 10\n",
    "mng.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
