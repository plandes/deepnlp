## Install the corpus
#
# declare resources to be downloaded
[feature_resource]
url = https://github.com/BobAdamsEE/SouthParkData/raw/master/All-seasons.csv
name = southpark.csv


## Feature creation
#
# overrides AutoSplitDataframeStash to create the dataframe from parsing the
# sentences from the corups; the parent class the splits the dataframe in to
# datasets using a column that gives the dataset name
[dataframe_stash]
class_name = sp.SouthParkDataframeStash
characters = list: stan, kyle, cartman

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing
[feature_factory_stash]
text_column = line
additional_columns = list: character

[feature_split_key_container]
partition_attr = label
stratified_write = True


## Batch
#
[batch_stash]
# map feature attributes (sections) to feature IDs to connect features to vectorizers
batch_feature_mappings = dataclass(zensols.deeplearn.batch.ConfigBatchFeatureMapping): sp_batch_mappings
decoded_attributes = set: label, ${sp_default:lang_features} ${sp_default:embedding}
workers = -2


## Natural language parsing
#
# override for creating instances of a class that have an attribute for the
# label of the text classification
[doc_parser]
doc_class = class: zensols.deepnlp.classify.LabeledFeatureDocument


## Application defaults
#
## vectorization
#
# override to provide the labels to vectorize
[classify_label_vectorizer]
categories = ${dataframe_stash:characters}


## Model
#
# let our decoder (last fully connected feed forward network) the output
# dimension as the number of labels to classify
[linear_settings]
out_features = eval: '${dataframe_stash:characters}'.count(',') + 1

# tell the model to use a feature prediction mapper for our classification
[model_settings]
model_name = ${sp_default:name}
learning_rate = 0.01
epochs = 20
prediction_mapper_name = classify_feature_prediction_mapper

# set the dropout for the classification network, which propogates down to the
# sub/owned networks
[classify_net_settings]
embedding_layer = instance: ${sp_default:embedding}_layer
recurrent_settings = instance: recurrent_settings
dropout = 0.1

# tell the model automation API which model to use
[executor]
net_settings = instance: classify_net_settings
