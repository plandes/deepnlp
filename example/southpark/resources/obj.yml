## Install the corpus
#
# declare resources to be downloaded
feature_resource:
  url: https://github.com/BobAdamsEE/SouthParkData/raw/master/All-seasons.csv
  name: southpark.csv


## Feature creation
#
# massages the CSV into a usable dataframe (only code in this project)
dataframe_stash:
  class_name: sp.SouthParkDataframeStash
  characters: 'list: stan, kyle, cartman'

# the stash of extracted language features in child processes for SpaCy parsing
feature_factory_stash:
  text_column: 'line'
  additional_columns: 'list: character'


## Batch
#
# map feature attributes (sections) to feature IDs to connect features to vectorizers
batch_stash:
  batch_feature_mappings: 'dataclass(zensols.deeplearn.batch.ConfigBatchFeatureMapping): sp_batch_mappings'
  condition:
    if: "eval: '${sp_default:name}'.startswith('transformer')"
    then:
      decoded_attributes: 'set: label, ${sp_default:embedding}'
    else:
      decoded_attributes: 'set: label, ${sp_default:lang_features} ${sp_default:embedding}'
  workers: -2

# batch mappings from attribute to feature IDs and which to use from resource libs
sp_batch_mappings:
  batch_feature_mapping_adds:
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): classify_label_batch_mappings'
    - 'dataclass(zensols.deeplearn.batch.BatchFeatureMapping): lang_batch_mappings'
  field_keep: [label, enums, dependencies, stats, counts, glove_50_embedding, transformer_trainable_embedding]


## Natural language parsing
#
# override for creating instances of a class that have an attribute for the
# label of the text classification
doc_parser:
  doc_class: "class: zensols.deepnlp.classify.LabeledFeatureDocument"

# override to provide the labels to vectorize
classify_label_vectorizer:
  categories: ${dataframe_stash:characters}


## Model
#
# tell the model automation API which model to use
executor:
  net_settings: 'instance: classify_net_settings'

# last dense network has an output dimension as number of labels
linear_settings:
  out_features: "eval: '${dataframe_stash:characters}'.count(',') + 1"

# tell the model to use a feature prediction mapper for our classification
0.condition:
  if: "eval: '${sp_default:name}'.startswith('transformer')"
  then:
    model_settings:
      model_name: 'transformer: ${transformer_trainable_resource:model_id}'
      learning_rate: 0.1
      epochs: 6
      prediction_mapper_name: classify_feature_prediction_mapper
  else:
    model_settings:
      model_name: '${sp_default:name}'
      learning_rate: 0.01
      epochs: 20
      prediction_mapper_name: classify_feature_prediction_mapper

# overrides for classification LSTM network
1.condition:
  if: "eval: '${sp_default:name}'.startswith('transformer')"
  then:
    classify_net_settings:
      embedding_layer: 'instance: ${sp_default:embedding}_layer'
      dropout: 0.1
  else:
    classify_net_settings:
      embedding_layer: 'instance: ${sp_default:embedding}_layer'
      recurrent_settings: 'instance: recurrent_settings'
      dropout: 0.1

2.condition:


transformer_trainable_resource:
  args:
    local_files_only: true


