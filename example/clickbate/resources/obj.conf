## Install the corpus
#
# declare resources to be downloaded
[cb_data_resource]
class_name = zensols.install.Resource
url = https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/clickbait_data.gz

[non_cb_data_resource]
class_name = zensols.install.Resource
url = https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/non_clickbait_data.gz

# the installer downloads and uncompresses the files
[installer]
class_name = zensols.install.Installer
downloader = object: zensols.install.Downloader
base_directory = path: ${default:corpus_dir}
resources = instance: list: cb_data_resource, non_cb_data_resource


## feature creation
#
# overrides AutoSplitDataframeStash to create the dataframe from parsing the
# sentences from the corups; the parent class the splits the dataframe in to
# datasets using a column that gives the dataset name
[dataframe_stash]
class_name = cb.corpus.ClickbateDataframeStash
installer = instance: installer
cb_data_resource = instance: cb_data_resource
non_cb_data_resource = instance: non_cb_data_resource
dataframe_path = path: ${default:data_dir}/feature/df.dat
distribution = dict: {'train': 0.70, 'validate': 0.15, 'test': 0.15}
split_col = split
# has the same keys as sampled from the dataframe, but pickled for fast read by
# `feature_stash` later when read by the model for the splits
key_path = path: ${default:data_dir}/feature/keys.dat

# a stash that uses a directory of pickled files to store the parsed (POS tags,
# NER tagged entities, etc.), which are later used to create features
[feature_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:data_dir}/parse

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing
[feature_factory_stash]
class_name = zensols.deepnlp.feature.DataframeDocumentFeatureStash
delegate = instance: feature_dir_stash
factory = instance: dataframe_stash
vec_manager = instance: language_feature_manager
chunk_size = 0
workers = 0
# uncomment to limit how many documents are parsed and later available on which
# to train
#document_limit = 10
text_column = sent
additional_columns = list: label

# a stash that splits along dataset type (i.e. train, validation, test)
[feature_stash]
class_name = zensols.dataset.DatasetSplitStash
delegate = instance: feature_factory_stash
split_container = instance: dataframe_stash

## natural language parsing
#
# override for creating instances of a class that have an attribute for the
# label of the text classification
[doc_parser]
doc_class = class: zensols.deepnlp.classify.LabeledFeatureDocument
# override to use the component
components = instance: list: remove_sent_boundaries_component



## Application defaults
#
## vectorization
#
# classes for the clickbate corpus
[class]
labels = y n

# override to provide the labels to vectorize
[classify_label_vectorizer]
categories = eval: '${class:labels}'.split()

# maintains a collection of all vectorizers for the framework
[vectorizer_manager_set]
names = list: language_feature_manager, classify_label_vectorizer_manager


## batch
#
[batch_stash]
# map feature attributes (sections) to feature IDs to connect features to vectorizers
batch_feature_mappings = dataclass(zensols.deeplearn.batch.ConfigBatchFeatureMapping): clickbate_batch_mappings
decoded_attributes = set: label, ${clickbate_default:lang_features} ${clickbate_default:embedding}
workers = -2


## model
#
# let our decoder (last fully connected feed forward network) the output
# dimension as the number of labels to classify
[linear_settings]
out_features = eval: len('${class:labels}'.split())

# declare (override) the ModelFacade to use for the application
[facade]
class_name = zensols.deepnlp.classify.ClassifyModelFacade

# tell the model to use a feature prediction mapper for our classification
[model_settings]
prediction_mapper_name = classify_feature_prediction_mapper

# set the dropout for the classification network, which propogates down to the
# sub/owned networks
[classify_net_settings]
dropout = 0.2

# tell the model automation API which model to use
[executor]
net_settings = instance: classify_net_settings

[deeplearn_default]
model_name = ${clickbate_default:embedding}
