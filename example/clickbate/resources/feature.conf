## Install the corpus
#
# declare resources to be downloaded
[cb_data_resource]
class_name = zensols.install.Resource
url = https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/clickbait_data.gz

[non_cb_data_resource]
class_name = zensols.install.Resource
url = https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/non_clickbait_data.gz

# the installer downloads and uncompresses the files
[installer]
class_name = zensols.install.Installer
downloader = object: zensols.install.Downloader
base_directory = path: ${default:corpus_dir}
installs = instance: list: cb_data_resource, non_cb_data_resource


## feature creation
#
# overrides AutoSplitDataframeStash to create the dataframe from parsing the
# sentences from the corups; the parent class the splits the dataframe in to
# datasets using a column that gives the dataset name
[dataframe_stash]
class_name = cb.corpus.ClickbateDataframeStash
installer = instance: installer
cb_data_resource = instance: cb_data_resource
non_cb_data_resource = instance: non_cb_data_resource
dataframe_path = path: ${default:data_dir}/feature/df.dat
distribution = dict: {'train': 0.70, 'validate': 0.15, 'test': 0.15}
split_col = split
# has the same keys as sampled from the dataframe, but pickled for fast read by
# `feature_stash` later when read by the model for the splits
key_path = path: ${default:data_dir}/feature/keys.dat

# a stash that uses a directory of pickled files to store the parsed (POS tags,
# NER tagged entities, etc.), which are later used to create features
[feature_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:data_dir}/parse

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing
[feature_factory_stash]
class_name = zensols.deepnlp.feature.DataframeDocumentFeatureStash
delegate = instance: feature_dir_stash
factory = instance: dataframe_stash
vec_manager = instance: language_feature_manager
chunk_size = 0
workers = 0
# uncomment to limit how many documents are parsed and later available on which
# to train
#document_limit = 10
text_column = sent
additional_columns = list: label

# a stash that splits along dataset type (i.e. train, validation, test)
[feature_stash]
class_name = zensols.dataset.DatasetSplitStash
delegate = instance: feature_factory_stash
split_container = instance: dataframe_stash
