## installed corpus
[cb_data_resource]
class_name = zensols.install.Resource
url = https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/clickbait_data.gz

[non_cb_data_resource]
class_name = zensols.install.Resource
url = https://github.com/bhargaviparanjape/clickbait/raw/master/dataset/non_clickbait_data.gz

[installer]
class_name = zensols.install.Installer
downloader = object: zensols.install.Downloader
base_directory = path: ${default:corpus_dir}
installs = instance: list: cb_data_resource, non_cb_data_resource


## feature
[dataframe_stash]
class_name = cb.corpus.ClickbateDataframeStash
installer = instance: installer
cb_data_resource = instance: cb_data_resource
non_cb_data_resource = instance: non_cb_data_resource
dataframe_path = path: ${default:data_dir}/feature/df.dat
distribution = dict: {'train': 0.85, 'validate': 0.15, 'test': 0.1}
split_col = split
key_path = path: ${default:data_dir}/feature/keys.dat


# directory to store the parsed (POS tags, NER tagged entities, etc.), which
# are later used to create features
[feature_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:data_dir}/parse

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing
[feature_factory_stash]
class_name = zensols.deepnlp.feature.DataframeDocumentFeatureStash
delegate = instance: feature_dir_stash
factory = instance: dataframe_stash
vec_manager = instance: language_feature_manager
chunk_size = 0
workers = 0
# uncomment to limit how many documents are parsed and later available on which
# to train
#document_limit = 10
text_column = sent
additional_columns = list: label

# a stash that splits along dataset type (i.e. train, validation, test)
[feature_stash]
class_name = zensols.dataset.DatasetSplitStash
delegate = instance: feature_factory_stash
split_container = instance: dataframe_stash
