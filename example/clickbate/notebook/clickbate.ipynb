{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning framework example: Clickbate dataset\n",
    "\n",
    "**Important**: Please see the [MNIST notebook example](https://github.com/plandes/deeplearn/blob/master/notebook/mnist.ipynb) in the [zensols.deeplearn](https://github.com/plandes/deeplearn) API first, as it contains more explaination of how the framework is used.\n",
    "\n",
    "See the [saved notebook](https://htmlpreview.github.io/?https://github.com/plandes/deepnlp/blob/master/example/clickbate/notebook/clickbate.html) to see the output of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environemnt configuration and set up: add this (deepnlp) library to the Python path and framework entry point\n",
    "from harness import NotebookHarness\n",
    "harness = NotebookHarness()\n",
    "mng = harness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print information about \n",
    "\n",
    "Use the factory to create the model executor.  The `write` method gives statistics on the data set that is configured on the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "# read the configuration from glove.conf using the same command line process to load the config and models\n",
    "facade = mng.create_facade('glove50')\n",
    "from zensols.config import Writable\n",
    "# set indention level for human readable (pretty print like) output\n",
    "Writable.WRITABLE_INDENT_SPACE = 2\n",
    "facade.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters\n",
    "\n",
    "Set model parameters to get a feel for where they need to be before changing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.epochs = 20\n",
    "facade.language_attributes = set()\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add linguistic features\n",
    "\n",
    "Add spaCy generated lingustic features by appending them to the embedding layer.  The enumerations are one-hot encoded vectors of POS tags, NER entities and dependenccy parent/child relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "facade.language_attributes = {'enums'}\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More linguistic features\n",
    "\n",
    "Add the syntactic dependency parser parent/child relationship as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade.language_attributes = {'enums', 'dependencies'}\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Now we'll try contextual BERT embeddings.  When we use `create_facade` it resets the language features and embedings to what's configured in `transformer.conf`.  In this case, it's a Bert cased model using the pooler output for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer')\n",
    "mng.run()\n",
    "facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa\n",
    "\n",
    "Try the RoBERTa model for comparison; this is done by overriding the ``model_id`` option on the ``transformer_resource`` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng.config('transformer_resource', model_id='roberta-base')\n",
    "facade = mng.create_facade('transformer')\n",
    "facade.epochs = 10\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "\n",
    "Try the DistilBERT model for comparison in the same way as RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mng.config('transformer_resource', model_id='distilbert-base-cased')\n",
    "facade = mng.create_facade('transformer')\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results\n",
    "\n",
    "Generate a dataframe with the performance metrics of the previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from zensols.deeplearn.result import ModelResultManager, ModelResultReporter\n",
    "rm: ModelResultManager = facade.result_manager\n",
    "reporter = ModelResultReporter(rm)\n",
    "reporter.dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
