## main
#
[default]
corpus_dir = ../../corpus


## language configuration
[filter_token_mapper]
class_name = zensols.nlp.FilterTokenMapper
remove_space = True

[map_filter_token_normalizer]
class_name = zensols.nlp.MapTokenNormalizer
mapper_class_list = eval: 'filter_token_mapper'.split()

[langres]
class_name = zensols.nlp.LanguageResource
lang = en
model_name = ${lang}_core_web_sm
token_normalizer = instance: map_filter_token_normalizer


## CPU based configuration
[torch_config]
class_name = zensols.deeplearn.TorchConfig
use_gpu = False


# creates features from documents by invoking by using SpaCy to parse the text
[doc_parser]
class_name = zensols.deepnlp.FeatureDocumentParser
langres = instance: langres
# indicate which features to keep after the parsing; if this is not given, all
# features are kept and persisted
#
# 'norm' is good for debuging, 'dep', 'children' and the rest are needed for
# dep head tree features
token_feature_ids = eval: set('ent_ dep_ norm ent dep tag tag_ children i dep_ is_punctuation idx i_sent'.split())


# creates counts of for each enumeration of the SpaCy feature parsed in
# `doc_parser`.
[count_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.CountEnumContainerFeatureVectorizer
feature_id = count
# the feature ids to use at train/test time, which currently are all those
# defined in `zensols.deepnlp.vectorize.SpacyFeatureVectorizer`, and include
# 'ent', 'tag', and 'dep'
#
# train time tweakable
decoded_feature_ids = eval: set('ent tag dep'.split())

# creates one hot vectors for each enumeration of the SpaCy feature parsed in
# `doc_parser`.
[enum_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.EnumContainerFeatureVectorizer
feature_id = enum
# the feature ids to use at train/test time, which currently are all those
# defined in `zensols.deepnlp.vectorize.SpacyFeatureVectorizer`, and include
# 'ent', 'tag', and 'dep'
#
# train time tweakable
decoded_feature_ids = eval: set('ent tag dep'.split())

# head tree dependency features
[depth_token_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.DepthFeatureDocumentVectorizer
feature_id = dep


## embedding
#
# glove embeddding model (not layer)
[glove_50_embedding]
class_name = zensols.deepnlp.embed.GloveWordEmbedModel
path = path: ${default:corpus_dir}/glove
desc = 6B
dimension = 50
lowercase = True

# a vectorizer that turns tokens (TokensContainer) in to indexes given to the
# embedding layer
[glove_50_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingFeatureVectorizer
# the feature id is used to connect instance data with the vectorizer used to
# generate the feature at run time
feature_id = wvglove50
embed_model = instance: glove_50_embedding
# do not serialize (pickle) the decoded output to do the work up front
encode_transformed = False

# a torch.nn.Module implementation that uses the an embedding model
[glove_50_embedding_layer]
class_name = zensols.deepnlp.layer.WordVectorEmbeddingLayer
embed_model = instance: glove_50_embedding
feature_vectorizer = instance: language_feature_manager

# transformer
[transformer_resource]
class_name = zensols.deepnlp.transformer.TransformerResource
torch_config = instance: torch_config
model_id = bert-base-cased
cased = True
# subproc CPU to CPU copy bug: https://github.com/huggingface/transformers/issues/8649
cache = True
# whether or not the embeddings are trainable
trainable = False

[transformer_tokenizer]
class_name = zensols.deepnlp.transformer.TransformerDocumentTokenizer
resource = instance: transformer_resource
word_piece_token_length = -1

[transformer_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbedding
tokenizer = instance: transformer_tokenizer
output = last_hidden_state

[transformer_feature_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingFeatureVectorizer
feature_id = transformer
embed_model = instance: transformer_embedding
encode_transformed = False

[transformer_embedding_layer]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingLayer
embed_model = instance: transformer_embedding
feature_vectorizer = instance: language_feature_manager

# mapper
[transformer_expander_vectorizer]
class_name = zensols.deepnlp.transformer.TransformerExpanderFeatureVectorizer
feature_id = transformer_expander
embed_model = instance: transformer_embedding
delegate_feature_ids = list: dep, enum
#delegate_feature_ids = list: dep


## manager
#
# a language specific vectorizer manager that uses the FeatureDocumentParser
# defined in `doc_parser` to create word embeddings using the vectorizer
# defined in `glove_50_feature_vectorizer` and natural language features
[language_feature_manager]
class_name = zensols.deepnlp.vectorize.FeatureDocumentVectorizerManager
torch_config = instance: torch_config
# word embedding vectorizers can not be class level since each instance is
# configured
configured_vectorizers = list: 
    count_feature_vectorizer,
    depth_token_feature_vectorizer,
    enum_feature_vectorizer,
    glove_50_feature_vectorizer,
    transformer_feature_vectorizer,
    transformer_expander_vectorizer
# used for parsing `FeatureDocument` instances
doc_parser = instance: doc_parser
# the number of tokens in the document to use
# token length is not one to one with parsed tokens when using BERT, 70 works well
token_length = -1
# features to use
token_feature_ids = ${doc_parser:token_feature_ids}
