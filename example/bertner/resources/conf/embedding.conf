## Embedding

# transformer
[transformer_embedding]
class_name = zensols.deepnlp.transformer.TransformerEmbeddingModel
torch_config = instance: gpu_torch_config
#model_name = distilbert
#model_name = bert
model_name = roberta
# the word piece length is always the same or greater in count than linguistic
# tokens because the word piece algorithm tokenizes on characters; however, in
# this case, we set it to the shared value `token_length` as the subsequent
# layer expects this as a dimension
word_piece_token_length = ${language_defaults:token_length}
#word_piece_token_length = 70
cased = True
# cache the model at the global level since there is only one huggingface
# transformer model we're using; also without it, subprocess CPU to CPU copy freezes:
# https://github.com/huggingface/transformers/issues/8649
cache = True
# from_pretrain extra arguments
pretrain_args = dict: {'local_files_only': True}
# whether or not the embeddings are trainable (not frozen)
trainable = True

[transformer_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.TransformerSentenceFeatureVectorizer
feature_id = transformer
embed_model = instance: transformer_embedding
as_document = True
encode_transformed = False

[transformer_embedding_layer]
class_name = zensols.deepnlp.vectorize.TransformerEmbeddingLayer
embed_model = instance: transformer_embedding
feature_vectorizer = instance: language_feature_manager
