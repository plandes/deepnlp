{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning framework example: Movie Review Dataset\n",
    "\n",
    "This notebook demonstrates how to use the deeplearning API to train and test the model on the [Stanford movie review corpus](https://nlp.stanford.edu/sentiment/) corpus.  This dataset contains hand written digits and their labels.  See the [saved version](https://htmlpreview.github.io/?https://github.com/plandes/deepnlp/blob/master/example/movie/notebook/movie.html) for output.\n",
    "\n",
    "**Important**: Please see the [Clickbate notebook example](https://github.com/plandes/deepnlp/blob/master/example/clickbate/notebook/clickbate.ipynb) for a more simple example of how to utilize the framework to tune pamateres in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# environemnt configuration and set up: add this (deepnlp) library to the Python path and framework entry point\n",
    "from mngfac import JupyterManagerFactory\n",
    "fac = JupyterManagerFactory()\n",
    "mng = fac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print information about \n",
    "\n",
    "Use the factory to create the model executor.  The `write` method gives statistics on the data set that is configured on the executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "facade = mng.create_facade()\n",
    "mng.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the first test\n",
    "\n",
    "Set the number of epochs to a value that might be different from the default in the configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facade.epochs = 15\n",
    "mng.run()\n",
    "# uncomment to persist the plot and results to the file system\n",
    "#facade.persist_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try word2vec embeddings\n",
    "\n",
    "Change the embeddings to word2vec and rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the embedding updates the model, but currently results show initial loaded embeddings (glove50)\n",
    "facade.embedding = 'word2vec_300_embedding'\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer fixed embeddings\n",
    "\n",
    "Next try frozen Bert embeddings.  These are computed from the dataset and used as fixed embeddings (rather than being trainable fine-tuned embeddings).  Note the performance isn't much improved since the real strength of a transformer architecture is being able to fine tune it to our sentient task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# when comparing several models with the same embeddings with different language\n",
    "# features (later cells), recreate to get a consistent random seed and clean state\n",
    "facade = mng.create_facade('transformer-fixed')\n",
    "# since the configuration files already have a default linguistic feature set, clear it out\n",
    "facade.language_attributes = set()\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer-fixed')\n",
    "facade.language_attributes = {'transformer_enum_expander'}\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a transformer expander\n",
    "\n",
    "A vectorizer that expands lingustic feature vectors to their respective locations as word piece token vectors.  This is used to concatenate lingustic features with Bert (and other transformer) embeddings.  Each lingustic token is copied in the word piece token location across all vectorizers and sentences.\n",
    "\n",
    "First we'll try the dependency expander, which adds the spaCy head three depth feature to the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer-fixed')\n",
    "facade.language_attributes = {'transformer_dep_expander'}\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add enumerated features\n",
    "\n",
    "Next try the enumeration expander, which adds the spaCy enumerated parsed values (i.e. POS and NER tags) as features to the corresponding wordpiece tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer-fixed')\n",
    "facade.language_attributes = {'transformer_enum_expander', 'transformer_dep_expander'}\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert fine tuning\n",
    "\n",
    "Finally, try fine tuning the Bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer-trainable')\n",
    "mng.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust optimizer and schedule\n",
    "\n",
    "Decay the weights in the optimizer and drop the learning rate after 3 validation loss drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "facade = mng.create_facade('transformer-trainable')\n",
    "facade.model_settings.optimizer_params = {'weight_decay': 5e-3}\n",
    "facade.model_settings.scheduler_params = {'patience': 3}\n",
    "facade.epochs = 14\n",
    "mng.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
