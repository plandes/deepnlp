## Natural language parsing and features
[language_defaults]
token_length = 56
embedding = ${model_defaults:embedding}
#embedding = here

# down case all tokens
[lower_case_token_mapper]
class_name = zensols.nlp.LambdaTokenMapper
map_lambda = lambda x: (x[0], x[1].lower())

# remove all white space tokens
[filter_token_mapper]
class_name = zensols.nlp.FilterTokenMapper
remove_space = True

# a token normalizer that aggregates mapper functionality
[token_normalizer]
class_name = zensols.nlp.MapTokenNormalizer
mapper_class_list = eval: 'filter_token_mapper'.split()

# language resource, which loads the SpacCy model and token normalizers
[langres]
class_name = zensols.nlp.LanguageResource
token_normalizer = instance: token_normalizer

# creates features from documents by invoking by using SpaCy to parse the text
[doc_parser]
class_name = zensols.deepnlp.FeatureDocumentParser
langres = instance: langres
# indicate which features to keep after the parsing; if this is not given, all
# features are kept and persisted
#
# 'norm' is good for debuging, 'dep', 'children' and the rest are needed for
# dep head tree features
token_feature_ids = eval: set('norm ent dep tag children i dep_ is_punctuation'.split())

# creates one hot vectors for each enumeration of the SpaCy feature parsed in
# `doc_parser`.
[enum_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.EnumContainerFeatureVectorizer
feature_id = enum
# the feature ids to use at train/test time, which currently are all those
# defined in `zensols.deepnlp.vectorize.SpacyFeatureVectorizer`, and include
# 'ent', 'tag', and 'dep'
#
# train time tweakable
decoded_feature_ids = eval: set('ent tag dep'.split())

# creates counts of for each enumeration of the SpaCy feature parsed in
# `doc_parser`.
[count_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.CountEnumContainerFeatureVectorizer
feature_id = count
# the feature ids to use at train/test time, which currently are all those
# defined in `zensols.deepnlp.vectorize.SpacyFeatureVectorizer`, and include
# 'ent', 'tag', and 'dep'
#
# train time tweakable
decoded_feature_ids = eval: set('ent tag dep'.split())

# language statistic (no of tokens, etc)
[language_stats_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.StatisticsTokenContainerFeatureVectorizer
feature_id = stats

# head tree dependency features
[depth_token_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.DepthTokenContainerFeatureVectorizer
feature_id = dep

# a language specific vectorizer manager that uses the FeatureDocumentParser
# defined in `doc_parser` to create word embeddings using the vectorizer
# defined in `glove_50_feature_vectorizer` and natural language features
[language_feature_manager]
class_name = zensols.deepnlp.vectorize.TokenContainerFeatureVectorizerManager
torch_config = instance: gpu_torch_config
# word embedding vectorizers can not be class level since each instance is
# configured
configured_vectorizers = eval: [
  'word2vec_300_feature_vectorizer',
  'glove_50_feature_vectorizer',
  'glove_300_feature_vectorizer',
  'transformer_feature_vectorizer',
  'enum_feature_vectorizer',
  'count_feature_vectorizer',
  'language_stats_feature_vectorizer',
  'depth_token_feature_vectorizer']
# used for parsing `FeatureDocument` instances
doc_parser = instance: doc_parser
# the number of tokens in the document to use
# token length is not one to one with parsed tokens when using BERT, 70 works well
token_length = ${language_defaults:token_length}
# features to use
token_feature_ids = ${doc_parser:token_feature_ids}
