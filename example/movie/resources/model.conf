## Model configuration, settings and hyperparameters

# the executor uses PyTorch to train, test and validate the model itself; it
# also saves the results and model
[executor]
class_name = zensols.deeplearn.model.ModelExecutor
# human readable text
model_name = eval: 'Review: ${model_defaults:embedding}'.replace('_embedding', ' ')
# configures the model
model_settings = instance: model_settings
# configures the neural network
net_settings = instance: net_settings
# stash to get the batch data
dataset_stash = instance: dataset_stash
# the datasets by name found in `dataset_stash`
# *Important:* order must be: training, validation/development, test
dataset_split_names = eval: 'train dev test'.split()
# the pah the store the results after completing training or testing
result_path = path: ${default:results_dir}/model
# add results while training the model
intermediate_results_path = path: ${default:temporary_dir}/tmpres
# use regression error metrics to evaluate the results
#reduce_outcomes = none
# the path to watch for early stopping
update_path = path: ${default:temporary_dir}/update.json


## user tweeks

[recurrent_settings]
class_name = zensols.deeplearn.layer.RecurrentAggregationNetworkSettings
# the type of network (one of `rnn`, `lstm`, `gru`)
network_type = lstm
# the type of aggregation of the layer, one of `max`, `ave`, `last`
aggregation = max
# the input size, but set to None since this is set from the embedding layer
# metadata
input_size = None
# hidden LSTM dimension
hidden_size = 36
# "stacked" LSTM
num_layers = 1
# whether or the LSTM is stacked
bidirectional = True
# set by root level settings
dropout = None

[linear_settings]
class_name = zensols.deeplearn.layer.DeepLinearNetworkSettings
# number deep linear layers configured after the LSTM
middle_features = eval: [10]
# number of output features
out_features = eval: len('${class:labels}'.split())
# the number of input features to the deep linear layer; set to null since
# calculated in the model
in_features = None
# whether to treat each middle layer as a scalar multiplier of the previous or
# to interpret them as a constant number of parameters
proportions = True
# number of times to repeat the middle layers
repeats = 1
# drop out used for the middle layers (set by root level settings)
dropout = None
# no activation used in this set of layers
activation = None
# the dimension of the batch normalization to use or None
batch_norm_d = None
# number of features C used by the norm or `None` if not used; where C from an
# expected input of size (N, C, L) or L from input of size (N, L)
batch_norm_features = None

# the network configuration, which contains constant information (as opposed to
# dynamic configuration such as held back `stash:decoded_attributes`)
[net_settings]
class_name = movie.ReviewNetworkSettings
# embedding layer used as the input layer
embedding_layer = instance: ${language_defaults:embedding}_layer
# the recurrent neural network after the embeddings
recurrent_settings = None
# the (potentially) deep linear network
linear_settings = instance: linear_settings
# metadata factory helps configure the network (see that configuration)
batch_metadata_factory = instance: batch_metadata_factory
# sets the dropout for the network
dropout = 0.2

# model specific configuration, mutually exclusive from neural network details
[model_settings]
class_name = zensols.deeplearn.model.ModelSettings
# path where the model is saved on each validation decrease
path = path: ${default:temporary_dir}/model/${language_defaults:embedding}
# learning rate set on the optimizer
learning_rate = 0.001
# how the batches are buffered; one of `gpu`, which buffers all data in the
# GPU, `cpu`, which means keep all batches in CPU memory (the default), or
# `buffered` which means to buffer only one batch at a time (only for *very*
# large data) how to batch data: gpu to load all in to the GPU,
#
# its a tiny data set so it will literally all fit in GPU memory
batch_iteration = gpu
# number of epochs to train the model
epochs = 2
# the maximum number of times the validation loss can decrease per epoch before
# the executor "gives up" and ends training
#max_consecutive_increased_count = 5
# indicates the frequency by with the Python garbage collector should be
# invoked: 0: never 1: before and after training or testing 2: after each epoch
# 3: after each batch
#gc_level = 1
# optimizer
#optimizer_class_name = torch.optim.SGD
#criterion_class_name = torch.nn.MSELoss
# learning rate scheduler
scheduler_class_name = torch.optim.lr_scheduler.ReduceLROnPlateau
# number of batches to limit for train, test and validation, which is used for
# debugging
#batch_limit = 2
