## Install the corpus: declare resources to be downloaded
#
# stanford corpus resource
[mr_standford_resource]
class_name = zensols.install.Resource
url = https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip
name = stanfordSentimentTreebank
clean_up = False

# polarity labels resource
[mr_cornell_resource]
class_name = zensols.install.Resource
url = https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz
name = rt-polaritydata
clean_up = False

# the installer downloads and uncompresses the files
[mr_installer]
class_name = zensols.install.Installer
downloader = object: zensols.install.Downloader
base_directory = path: ${default:corpus_dir}
resources = instance: list: mr_standford_resource, mr_cornell_resource



## Project specific corpus
#
# configuration needed to create the corpus and a Pandas dataframe used by the
# framework
[dataset_factory]
class_name = dataset.DatasetFactory
installer = instance: mr_installer
# stanford corpus resource
standford_resource = instance: mr_standford_resource
# polarity labels resource
cornell_resource = instance: mr_cornell_resource
# path to input dataset CSV corpus files
dataset_path = path: ${default:corpus_dir}/dataset
# total number of tokens for each sentence key
tok_len = 10
# characters and strings to remove from the sentence key
throw_out = set("`` '' \" ` ' ( ) [ ] -lrb- -rrb- \/ / --".split())
# sentence string replacement
repls = eval: [['cannot', 'can not'], [" n't", "n't"]]
# column used for splits
split_col = split



## Feature stashes
#
# a stash of rows from a Pandas dataset for each movie review
[dataframe_stash]
class_name = domain.ReviewRowStash
dataset_factory = instance: dataset_factory
# location of pickled cache data to avoid recreating the dataframe each time
dataframe_path = path: ${default:data_dir}/feature/df.dat
# has the same keys as sampled from the dataframe, but pickled for fast read by
# `feature_stash` later when read by the model for the splits
key_path = path: ${default:data_dir}/feature/keys.dat
# column name of the data st split
split_col = ${dataset_factory:split_col}

# directory to store the parsed (POS tags, NER tagged entities, etc.), which
# are later used to create features
[feature_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:data_dir}/parse

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing on each
# utterance in the review
[feature_factory_stash]
class_name = domain.ReviewFeatureStash
delegate = instance: feature_dir_stash
factory = instance: dataframe_stash
vec_manager = instance: language_feature_manager
chunk_size = 0
workers = 0

# create stratified datasets along the label
[feature_split_key_container]
class_name = zensols.dataset.StratifiedStashSplitKeyContainer
# the stash to stratify
stash = instance: feature_factory_stash
# distribution of each data set
distribution = dict: {'train': 0.8, 'test': 0.1, 'validation': 0.1}
# per dataset file name with newline separated keys
pattern = {name}.txt
key_path = path: ${default:data_dir}/feature/dataset-row-ids
split_labels_path = path: ${default:data_dir}/feature/strat-dataframe.dat
# the attribute on each data point (movie review) to create the stratas
partition_attr = polarity
# show per strata statistics (takes longer on `write`)
stratified_write = True

# a stash that splits along dataset type (i.e. train, validation, test)
[feature_stash]
class_name = zensols.dataset.DatasetSplitStash
delegate = instance: feature_factory_stash
split_container = instance: feature_split_key_container
