## Install the corpus: declare resources to be downloaded
#
# stanford corpus resource
[mr_standford_resource]
class_name = zensols.install.Resource
url = https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip
name = stanfordSentimentTreebank
clean_up = False

# polarity labels resource
[mr_cornell_resource]
class_name = zensols.install.Resource
url = https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz
name = rt-polaritydata
clean_up = False

# the installer downloads and uncompresses the files
[mr_installer]
class_name = zensols.install.Installer
downloader = object: zensols.install.Downloader
base_directory = path: ${default:corpus_dir}
resources = instance: list: mr_standford_resource, mr_cornell_resource



## Project specific corpus
#
# configuration needed to create the corpus and a Pandas dataframe used by the
# framework
[dataset_factory]
class_name = mr.DatasetFactory
installer = instance: mr_installer
# stanford corpus resource
standford_resource = instance: mr_standford_resource
# polarity labels resource
cornell_resource = instance: mr_cornell_resource
# path to input dataset CSV corpus files
dataset_path = path: ${default:corpus_dir}/dataset
# total number of tokens for each sentence key
tok_len = 10
# characters and strings to remove from the sentence key
throw_out = set("`` '' \" ` ' ( ) [ ] -lrb- -rrb- \/ / --".split())
# sentence string replacement
repls = eval: [['cannot', 'can not'], [" n't", "n't"]]
# column used for splits
split_col = split



## Feature stashes
#
# a stash of rows from a Pandas dataset for each movie review
[dataframe_stash]
class_name = mr.MovieReviewRowStash
dataset_factory = instance: dataset_factory
# location of pickled cache data to avoid recreating the dataframe each time
dataframe_path = path: ${default:data_dir}/feature/df.dat
# has the same keys as sampled from the dataframe, but pickled for fast read by
# `feature_stash` later when read by the model for the splits
key_path = path: ${default:data_dir}/feature/keys.dat
# column name of the data st split
split_col = ${dataset_factory:split_col}

# directory to store the parsed (POS tags, NER tagged entities, etc.), which
# are later used to create features
[feature_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:data_dir}/parse

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing on each
# utterance in the review
[feature_factory_stash]
class_name = mr.MovieReviewFeatureStash
delegate = instance: feature_dir_stash
factory = instance: dataframe_stash
vec_manager = instance: language_feature_manager
chunk_size = 0
workers = 0

# create stratified datasets along the label
[feature_split_key_container]
class_name = zensols.dataset.StratifiedStashSplitKeyContainer
# the stash to stratify
stash = instance: feature_factory_stash
# distribution of each data set
distribution = dict: {'train': 0.8, 'test': 0.1, 'validation': 0.1}
# per dataset file name with newline separated keys
pattern = {name}.txt
key_path = path: ${default:data_dir}/feature/dataset-row-ids
split_labels_path = path: ${default:data_dir}/feature/strat-dataframe.dat
# the attribute on each data point (movie review) to create the stratas
partition_attr = polarity
# show per strata statistics (takes longer on `write`)
stratified_write = True

# a stash that splits along dataset type (i.e. train, validation, test)
[feature_stash]
class_name = zensols.dataset.DatasetSplitStash
delegate = instance: feature_factory_stash
split_container = instance: feature_split_key_container


## Vectorization
#
# classes for the clickbate corpus
[class]
labels = n p

# override to provide the labels to vectorize
[classify_label_vectorizer]
categories = eval: '${class:labels}'.split()

# maintains a collection of all vectorizers for the framework
[vectorizer_manager_set]
names = list: language_feature_manager, transformer_expander_feature_manager, classify_label_vectorizer_manager



## Batch
#
[batch_stash]
data_point_type = eval({'import': ['mr']}): mr.MovieReviewDataPoint
# map feature attributes (sections) to feature IDs to connect features to vectorizers
batch_feature_mappings = dataclass(zensols.deeplearn.batch.ConfigBatchFeatureMapping): mr_batch_mappings
decoded_attributes = set: label, ${mr_default:lang_features} ${mr_default:embedding}
workers = 2



## Model
#
# let our decoder (last fully connected feed forward network) the output
# dimension as the number of labels to classify
[linear_settings]
out_features = eval: len('${class:labels}'.split())

# declare (override) the ModelFacade to use for the application
[facade]
class_name = zensols.deepnlp.classify.ClassifyModelFacade

# tell the model to use a feature prediction mapper for our classification
[model_settings]
prediction_mapper_name = classify_feature_prediction_mapper

# set the dropout for the classification network, which propogates down to the
# sub/owned networks
[classify_net_settings]
dropout = 0.2

# tell the model automation API which model to use
[executor]
net_settings = instance: classify_net_settings

[deeplearn_default]
model_name = ${mr_default:embedding}



## Prediction mapper
#
# create data points from the client
[classify_feature_prediction_mapper]
pred_attribute = polarity
softmax_logit_attribute = confidence
