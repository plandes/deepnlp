## Batches used for training the model.  This configures the classes that
## groups each data time (in our example movie reviews) in to files for fast
## retrieval later.

# a stash that groups features across directories, with each directory
# containing batch files of the respective feature group
[batch_dir_stash]
class_name = zensols.deeplearn.batch.BatchDirectoryCompositeStash
# top level directory to store each feature sub directory
path = path: ${default:batch_dir}/data
groups = eval: (
       # there will be N (batch_stash:batch_size) batch labels in one file in a
       # directory of just label files
       set('label'.split()),
       # because we might want to switch between embeddings, separate them
       set('glove_50_embedding'.split()),
       set('glove_300_embedding'.split()),
       set('word2vec_300_embedding'.split()),
       set('transformer_embedding'.split()),
       # however, natural language features are optional for this task
       set('enums stats counts dependencies'.split()))

# a stash of Batch instances given to the model during training, validation and
# testing; this class spawns sub processes to concatenate arrays of features in
# to batches containing the tensors consumed by the model
[batch_stash]
class_name = zensols.deeplearn.batch.BatchStash
delegate = instance: batch_dir_stash
# this stash is used to generate instances of what will be used to create batches
split_stash_container = instance: feature_stash
# where to store the keys as mutually exclusive across dataset (train vs test etc)
data_point_id_sets_path = path: ${default:batch_dir}/batch-point-keys.dat
# indicate what will be used to vectorize in to tensors from features
vectorizer_manager_set = instance: vectorizer_manager_set
# the class that contains the feature data, one for each data instance
data_point_type = eval({'import': ['movie']}): movie.ReviewDataPoint
# the class taht contains the batch data, which will have N instances of
# `data_point_type` where N is the `batch_size`
batch_type = eval({'import': ['movie']}): movie.ReviewBatch
# the attributes used only on loading; all features indicated in the vectorizer
# manager (i.e. `language_feature_manager`) are saved; this makes it fast to
# try different feature combinations without havign to re-vectorize the entire
# dataset; if this is set to `None`, use all attributes given
#
# train time tweakable
decoded_attributes = eval: set('label ${language_defaults:embedding}'.split())
# the PyTorch configuration used to load batch data in to memory, in this case,
# the GPU if available
model_torch_config = instance: gpu_torch_config
# number of chunks of data given to each worker sub process; if 0 optimize for
# batch size and number of CPU cores
chunk_size = 0
# number sub processes; if 0, then the number of CPU cores
workers = 3
# the number of data instances per batch, and the first dimension of each
# tensor given to the model
#batch_size = 200
batch_size = 3
# limit on the number of batches per data set; typically multiply this by 3 to
# get a total count
#batch_limit = eval: sys.maxsize
batch_limit = 2

# stash to not only split data by dataset (i.e. train, test), but also sort the
# keys across all; which is important for reproducibility of results; this
# pulls from the `batch_stash`, so sorting is done only on the loaded data
[dataset_stash]
class_name = zensols.dataset.SortedDatasetSplitStash
delegate = instance: batch_stash
split_container = instance: batch_stash
sort_function = eval: int

# produces metadata configured and discovered in the `batch_stash` to tell the
# model of the dimensionality of given when creating the network
[batch_metadata_factory]
class_name = zensols.deeplearn.batch.BatchMetadataFactory
stash = instance: batch_stash
