## movie review demo configuration file
#
# This file contains the configuration needed to set up all in memory classes
# to train, validate and test a deep learning model


## Basic configuration

# root level directories with one root directory (root_dir)
[default]
root_dir = ${env:app_root}
corpus_dir = ${root_dir}/corpus
data_dir = ${root_dir}/data
temporary_dir = ${root_dir}/target
batch_dir = ${data_dir}/batch
results_dir = ${temporary_dir}/results


## PyTorch configuration indicates where to use the GPU vs CPU and default
## types

# CPU based configuration
[torch_config]
class_name = zensols.deeplearn.TorchConfig
use_gpu = False
data_type = eval({'import': ['torch']}): torch.float32

# GPU based confgiuration
[gpu_torch_config]
class_name = zensols.deeplearn.TorchConfig
use_gpu = True
data_type = eval({'import': ['torch']}): torch.float32



## Embedding

# glove embeddding model (not layer)
[glove_50_embedding]
class_name = zensols.deepnlp.embed.GloveWordEmbedModel
path = path: ${default:corpus_dir}/glove
desc = 6B
dimension = 50
lowercase = True

# a vectorizer that turns tokens (TokensContainer) in to indexes given to the
# embedding layer
[glove_50_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorSentenceFeatureVectorizer
# the feature id is used to connect instance data with the vectorizer used to
# generate the feature at run time
feature_id = wvglove50
embed_model = instance: glove_50_embedding
# treat the document as a stream of tokens generating a flat set of indexes
as_document = True
# decode in to the embedding matrix
#decode_embedding = True

# a torch.nn.Module implementation that uses the an embedding model
[glove_50_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: glove_50_embedding
feature_vectorizer = instance: language_feature_manager

# glove 300 dim
[glove_300_embedding]
class_name = zensols.deepnlp.embed.GloveWordEmbedModel
path = path: ${default:corpus_dir}/glove
desc = 6B
dimension = 300
lowercase = True

[glove_300_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorSentenceFeatureVectorizer
feature_id = wvglove300
embed_model = instance: glove_300_embedding
as_document = True

[glove_300_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: glove_300_embedding
feature_vectorizer = instance: language_feature_manager

# word2vec
[word2vec_300_embedding]
class_name = zensols.deepnlp.embed.Word2VecModel
path = path: ${default:corpus_dir}/word2vec/GoogleNews-vectors-negative300.bin
dimension = 300

[word2vec_300_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.WordVectorSentenceFeatureVectorizer
feature_id = w2v300
embed_model = instance: word2vec_300_embedding
as_document = True
decode_embedding = True

[word2vec_300_embedding_layer]
class_name = zensols.deepnlp.vectorize.WordVectorEmbeddingLayer
embed_model = instance: word2vec_300_embedding
feature_vectorizer = instance: language_feature_manager

# bert
[bert_embedding]
class_name = zensols.deepnlp.embed.BertEmbeddingModel
torch_config = instance: gpu_torch_config
#model_name = distilbert
model_name = roberta
cache_dir = path: ${default:corpus_dir}/bert

[bert_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.BertSentenceFeatureVectorizer
feature_id = bert
embed_model = instance: bert_embedding
as_document = True

[bert_embedding_layer]
class_name = zensols.deepnlp.vectorize.BertEmbeddingLayer
embed_model = instance: bert_embedding
feature_vectorizer = instance: language_feature_manager



## Natural language parsing and features
[language_defaults]
token_length = 56
embedding = glove_50_embedding
#embedding = word2vec_300_embedding

# down case all tokens
[lower_case_token_mapper]
class_name = zensols.nlp.LambdaTokenMapper
map_lambda = lambda x: (x[0], x[1].lower())

# remove all white space tokens
[filter_token_mapper]
class_name = zensols.nlp.FilterTokenMapper
remove_space = True

# a token normalizer that aggregates mapper functionality
[token_normalizer]
class_name = zensols.nlp.MapTokenNormalizer
#mapper_class_list = eval: 'filter_token_mapper lower_case_token_mapper'.split()
mapper_class_list = eval: 'filter_token_mapper'.split()

# language resource, which loads the SpacCy model and token normalizers
[langres]
class_name = zensols.nlp.LanguageResource
token_normalizer = instance: token_normalizer

# creates features from documents by invoking by using SpaCy to parse the text
[doc_parser]
class_name = zensols.deepnlp.FeatureDocumentParser
langres = instance: langres
# indicate which features to keep after the parsing; if this is not given, all
# features are kept and persisted
#
# 'norm' is good for debuging, 'dep', 'children' and the rest are needed for
# dep head tree features
token_feature_ids = eval: set('norm ent dep tag children i dep_ is_punctuation'.split())

# creates one hot vectors for each enumeration of the SpaCy feature parsed in
# `doc_parser`.
[enum_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.EnumContainerFeatureVectorizer
feature_id = enum
# the feature ids to use at train/test time, which currently are all those
# defined in `zensols.deepnlp.vectorize.SpacyFeatureVectorizer`, and include
# 'ent', 'tag', and 'dep'
#
# train time tweakable
decoded_feature_ids = eval: set('ent tag dep'.split())

# creates counts of for each enumeration of the SpaCy feature parsed in
# `doc_parser`.
[count_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.CountEnumContainerFeatureVectorizer
feature_id = count
# the feature ids to use at train/test time, which currently are all those
# defined in `zensols.deepnlp.vectorize.SpacyFeatureVectorizer`, and include
# 'ent', 'tag', and 'dep'
#
# train time tweakable
decoded_feature_ids = eval: set('ent tag dep'.split())

# language statistic (no of tokens, etc)
[language_stats_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.StatisticsTokenContainerFeatureVectorizer
feature_id = stats

# head tree dependency features
[depth_token_feature_vectorizer]
class_name = zensols.deepnlp.vectorize.DepthTokenContainerFeatureVectorizer
feature_id = dep

# a language specific vectorizer manager that uses the FeatureDocumentParser
# defined in `doc_parser` to create word embeddings using the vectorizer
# defined in `glove_50_feature_vectorizer` and natural language features
[language_feature_manager]
class_name = zensols.deepnlp.vectorize.TokenContainerFeatureVectorizerManager
torch_config = instance: torch_config
# word embedding vectorizers can not be class level since each instance is
# configured
configured_vectorizers = eval: [
  'word2vec_300_feature_vectorizer',
  'glove_50_feature_vectorizer',
  'glove_300_feature_vectorizer',
  'bert_feature_vectorizer',
  'enum_feature_vectorizer',
  'count_feature_vectorizer',
  'language_stats_feature_vectorizer',
  'depth_token_feature_vectorizer']
# used for parsing `FeatureDocument` instances
doc_parser = instance: doc_parser
# the number of tokens in the document to use
# token length is not one to one with parsed tokens when using BERT, 70 works well
token_length = ${language_defaults:token_length}
# features to use
token_feature_ids = ${doc_parser:token_feature_ids}



## Features and vectorization

# our task is binary: either a review is positive or negative
[class]
labels = n p

# vectorize the labels from text to PyTorch tensors
[label_vectorizer]
class_name = zensols.deeplearn.vectorize.NominalEncodedEncodableFeatureVectorizer
categories = eval: '${class:labels}'.split()
feature_id = rvlabel

# the vectorizer for labels is not language specific and lives in the
# zensols.deeplearn.vectorize package, so it needs it's own instance
[label_vectorizer_manager]
class_name = zensols.deeplearn.vectorize.FeatureVectorizerManager
torch_config = instance: torch_config
configured_vectorizers = eval: 'label_vectorizer'.split()

# maintains a collection of all vectorizers for the framework
[vectorizer_manager_set]
class_name = zensols.deeplearn.vectorize.FeatureVectorizerManagerSet
names = eval: 'language_feature_manager label_vectorizer_manager'.split()



## Project specific corpus

# configuration needed to create the corpus and a Pandas dataframe used by the
# framework (zensols.dep
[dataset_factory]
class_name = movie.DatasetFactory
# path to the stanford corpus
stanford_path = path: ${default:corpus_dir}/stanfordSentimentTreebank
# path to polarity labels
rt_pol_path = path: ${default:corpus_dir}/rt-polaritydata
# path to input dataset CSV corpus files
dataset_path = path: ${default:corpus_dir}/dataset
# total number of tokens for each sentence key
tok_len = 10
# characters and strings to remove from the sentence key
throw_out = set("`` '' \" ` ' ( ) [ ] -lrb- -rrb- \/ / --".split())
# sentence string replacement
repls = eval: [['cannot', 'can not'], [" n't", "n't"]]
# column used for splits
split_col = split

# a stash of rows from a Pandas dataset for each movie review
[dataframe_stash]
class_name = movie.ReviewRowStash
dataset_factory = instance: dataset_factory
# location of pickled cache data to avoid recreating the dataframe each time
dataframe_path = path: ${default:data_dir}/df.dat
split_col = ${dataset_factory:split_col}
key_path = path: ${default:data_dir}/keys.dat

# directory to store the parsed (POS tags, NER tagged entities, etc.), which
# are later used to create features
[feature_dir_stash]
class_name = zensols.persist.DirectoryStash
path = path: ${default:data_dir}/parse

# the stash of extracted natural language features derived from parsing; this
# is generated by spawning child processes to invoke SpaCy parsing on each
# utterance in the review
[feature_factory_stash]
class_name = movie.ReviewFeatureStash
delegate = instance: feature_dir_stash
factory = instance: dataframe_stash
vec_manager = instance: language_feature_manager
chunk_size = 0
workers = 0
document_limit = eval: sys.maxsize

# a stash that splits along dataset type (i.e. train, validation, test)
[feature_stash]
class_name = zensols.dataset.DatasetSplitStash
delegate = instance: feature_factory_stash
split_container = instance: dataframe_stash



## Batches used for training the model.  This configures the classes that
## groups each data time (in our example movie reviews) in to files for fast
## retrieval later.

# a stash that groups features across directories, with each directory
# containing batch files of the respective feature group
[batch_dir_stash]
class_name = zensols.deeplearn.batch.BatchDirectoryCompositeStash
# top level directory to store each feature sub directory
path = path: ${default:batch_dir}/data
groups = eval: (
       # there will be N (batch_stash:batch_size) batch labels in one file in a
       # directory of just label files
       set('label'.split()),
       # because we might want to switch between embeddings, separate them
       set('glove_50_embedding'.split()),
       set('glove_300_embedding'.split()),
       set('word2vec_300_embedding'.split()),
       set('bert_embedding'.split()),
       # however, natural language features are optional for this task
       set('enums stats counts dependencies'.split()))

# a stash of Batch instances given to the model during training, validation and
# testing; this class spawns sub processes to concatenate arrays of features in
# to batches containing the tensors consumed by the model
[batch_stash]
class_name = zensols.deeplearn.batch.BatchStash
delegate = instance: batch_dir_stash
# this stash is used to generate instances of what will be used to create batches
split_stash_container = instance: feature_stash
# where to store the keys as mutually exclusive across dataset (train vs test etc)
data_point_id_sets_path = path: ${default:batch_dir}/batch-point-keys.dat
# indicate what will be used to vectorize in to tensors from features
vectorizer_manager_set = instance: vectorizer_manager_set
# the class that contains the feature data, one for each data instance
data_point_type = eval({'import': ['movie']}): movie.ReviewDataPoint
# the class taht contains the batch data, which will have N instances of
# `data_point_type` where N is the `batch_size`
batch_type = eval({'import': ['movie']}): movie.ReviewBatch
# the attributes used only on loading; all features indicated in the vectorizer
# manager (i.e. `language_feature_manager`) are saved; this makes it fast to
# try different feature combinations without havign to re-vectorize the entire
# dataset; if this is set to `None`, use all attributes given
#
# train time tweakable
decoded_attributes = eval: set('label ${language_defaults:embedding}'.split())
# the PyTorch configuration used to load batch data in to memory, in this case,
# the GPU if available
model_torch_config = instance: gpu_torch_config
# number of chunks of data given to each worker sub process; if 0 optimize for
# batch size and number of CPU cores
chunk_size = 0
# number sub processes; if 0, then the number of CPU cores
workers = 3
# the number of data instances per batch, and the first dimension of each
# tensor given to the model
batch_size = 200
# limit on the number of batches per data set; typically multiply this by 3 to
# get a total count
batch_limit = eval: sys.maxsize
# workers = 1
# batch_size = 2
# batch_limit = 2

# stash to not only split data by dataset (i.e. train, test), but also sort the
# keys across all; which is important for reproducibility of results; this
# pulls from the `batch_stash`, so sorting is done only on the loaded data
[dataset_stash]
class_name = zensols.dataset.SortedDatasetSplitStash
delegate = instance: batch_stash
split_container = instance: batch_stash
sort_function = eval: int

# produces metadata configured and discovered in the `batch_stash` to tell the
# model of the dimensionality of given when creating the network
[batch_metadata_factory]
class_name = zensols.deeplearn.batch.BatchMetadataFactory
stash = instance: batch_stash



## Model configuration, settings and hyperparameters

# the executor uses PyTorch to train, test and validate the model itself; it
# also saves the results and model
[executor]
class_name = zensols.deeplearn.model.ModelExecutor
# human readable text
model_name = Review
# configures the model
model_settings = instance: model_settings
# configures the neural network
net_settings = instance: net_settings
# stash to get the batch data
dataset_stash = instance: dataset_stash
# the datasets by name found in `dataset_stash`
# *Important:* order must be: training, validation/development, test
dataset_split_names = eval: 'train dev test'.split()
# the pah the store the results after completing training or testing
result_path = path: ${default:results_dir}
# add results while training the model
intermediate_results_path = path: ${default:temporary_dir}/tmpres
# the path to watch for early stopping
update_path = path: ${default:temporary_dir}/update.json

## user tweeks

[recurrent_settings]
class_name = zensols.deeplearn.layer.RecurrentAggregationNetworkSettings
# the type of network (one of `rnn`, `lstm`, `gru`)
network_type = lstm
# the type of aggregation of the layer, one of `max`, `ave`, `last`
aggregation = max
# the input size, but set to None since this is set from the embedding layer
# metadata
input_size = None
# hidden LSTM dimension
hidden_size = 36
# "stacked" LSTM
num_layers = 1
# whether or the LSTM is stacked
bidirectional = True
# set by root level settings
dropout = None

[linear_settings]
class_name = zensols.deeplearn.layer.DeepLinearNetworkSettings
# number deep linear layers configured after the LSTM
middle_features = eval: [10]
# number of output features
out_features = eval: len('${class:labels}'.split())
# the number of input features to the deep linear layer; set to null since
# calculated in the model
in_features = None
# whether to treat each middle layer as a scalar multiplier of the previous or
# to interpret them as a constant number of parameters
proportions = True
# number of times to repeat the middle layers
repeats = 1
# drop out used for the middle layers (set by root level settings)
dropout = None
# no activation used in this set of layers
activation = None
# the dimension of the batch normalization to use or None
batch_norm_d = None
# number of features C used by the norm or `None` if not used; where C from an
# expected input of size (N, C, L) or L from input of size (N, L)
batch_norm_features = None

# the network configuration, which contains constant information (as opposed to
# dynamic configuration such as held back `stash:decoded_attributes`)
[net_settings]
class_name = movie.ReviewNetworkSettings
# embedding layer used as the input layer
embedding_layer = instance: ${language_defaults:embedding}_layer
#embedding_layer = instance: word2vec_300_embedding_layer
# the recurrent neural network after the embeddings
recurrent_settings = instance: recurrent_settings
# the (potentially) deep linear network
linear_settings = instance: linear_settings
# metadata factory helps configure the network (see that configuration)
batch_metadata_factory = instance: batch_metadata_factory
# sets the dropout for the network
dropout = 0.2

# model specific configuration, mutually exclusive from neural network details
[model_settings]
class_name = zensols.deeplearn.model.ModelSettings
# path where the model is saved on each validation decrease
path = path: ${default:temporary_dir}/model
# learning rate set on the optimizer
learning_rate = 0.001
# how the batches are buffered; one of `gpu`, which buffers all data in the
# GPU, `cpu`, which means keep all batches in CPU memory (the default), or
# `buffered` which means to buffer only one batch at a time (only for *very*
# large data) how to batch data: gpu to load all in to the GPU,
#
# its a tiny data set so it will literally all fit in GPU memory
batch_iteration = gpu
# number of epochs to train the model
epochs = 3
# the maximum number of times the validation loss can decrease per epoch before
# the executor "gives up" and ends training
#max_consecutive_increased_count = 5
# indicates the frequency by with the Python garbage collector should be
# invoked: 0: never 1: before and after training or testing 2: after each epoch
# 3: after each batch
#gc_level = 1
# optimizer
#optimizer_class_name = torch.optim.SGD
# number of batches to limit for train, test and validation, which is used for
# debugging
#batch_limit = 1
